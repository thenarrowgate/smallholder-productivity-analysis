fa_b <- tryCatch(
fa(Rb,
nfactors = k,
fm       = "minres",
rotate   = "geominQ",
n.obs    = nrow(samp)
),
error = function(e) NULL
)
if (is.null(fa_b)) next
# d) compute φ and H
Lb    <- fa_b$loadings
phi_b <- diag(factor.congruence(Lambda0, Lb))
uniqs <- 1 - rowSums(Lb[]^2)
H_b   <- vapply(seq_len(k), function(j) {
num <- sum(Lb[, j])^2
num / (num + sum(uniqs))
}, numeric(1))
# success — return a single row of length 2*k
return(c(phi_b, H_b))
}
}
# tear down
close(pb)
stopCluster(cl)
# 5. unpack results
phis_rob <- res[,        1:k,    drop = FALSE]
Hs_rob   <- res[, (k+1):(2*k),    drop = FALSE]
# 6. summarize
phi_mean <- colMeans(phis_rob)
H_mean   <- colMeans(Hs_rob)
cat(sprintf("Finished %d valid bootstraps\n", nrow(phis_rob)))
cat("Robust mean Tucker's φ: ", phi_mean, "\n")
cat("Robust mean Hancock's H:",  H_mean,   "\n")
k     <- 4#k_MAP  # choose k
# Step 10 ─ Bootstrap robust MINRES+oblimin to get loadings & uniquenesses
p <- ncol(df_mix2_clean)
B <- 1000
n_cores <- parallel::detectCores() - 1
cl <- makeCluster(n_cores); registerDoSNOW(cl)
pb <- txtProgressBar(max=B, style=3)
opts <- list(progress = function(n) setTxtProgressBar(pb, n))
boot_load <- foreach(b=1:B, .combine=rbind,
.packages=c("psych","polycor"),
.options.snow=opts) %dopar% {
repeat {
samp <- df_mix2_clean[sample(nrow(df_mix2_clean), replace=TRUE), ]
Rb   <- tryCatch(hetcor(samp)$correlations, error=function(e) NULL)
if(is.null(Rb) || any(is.na(Rb))) next
fa_b <- tryCatch(fa(Rb, nfactors=k, fm="minres", rotate="geominQ", n.obs=nrow(samp)),
error=function(e) NULL)
if(is.null(fa_b)) next
return(c(as.vector(fa_b$loadings[]), fa_b$uniquenesses))
}
}
close(pb); stopCluster(cl)
# Step 11 ─ Summarize bootstrap: medians & 95% CIs
lambda_boot <- boot_load[, 1:(p*k)]
psi_boot    <- boot_load[, (p*k+1):(p*k+p)]
L_median    <- matrix(apply(lambda_boot, 2, median), nrow=p, ncol=k)
L_ci        <- apply(lambda_boot, 2, quantile, c(.025,.975))
psi_median  <- apply(psi_boot, 2, median)
# name dimensions
vars <- colnames(df_mix2_clean)
rownames(L_median) <- vars
colnames(L_median) <- paste0("F", 1:k)
names(psi_median)  <- vars
# reshape CIs
ci_arr     <- array(L_ci, dim=c(2,p,k))
df_L_ci    <- data.frame(
variable = rep(vars, each=k),
factor   = rep(colnames(L_median), times=p),
lower    = as.vector(ci_arr[1,,]),
upper    = as.vector(ci_arr[2,,]),
stringsAsFactors=FALSE
)
# build df_psi_ci for the uniqueness‐CI
psi_ci   <- apply(psi_boot, 2, quantile, c(.025, .975))
df_psi_ci <- data.frame(
variable = vars,
lower    = psi_ci[1, ],
upper    = psi_ci[2, ],
stringsAsFactors = FALSE
)
# build df_psi_ci for the uniqueness‐CI
psi_ci   <- apply(psi_boot, 2, quantile, c(.025, .975))
df_psi_ci <- data.frame(
variable = vars,
lower    = psi_ci[1, ],
upper    = psi_ci[2, ],
stringsAsFactors = FALSE
)
# Write medians and CIs to CSV
write.csv(L_median,        "L_median_sngl.csv",   row.names = TRUE)
write.csv(df_L_ci,         "L_ci_long_sngl.csv",  row.names = FALSE)
write.csv(data.frame(variable = vars,
psi_median = psi_median),
"psi_median_sngl.csv", row.names = FALSE)
write.csv(df_psi_ci,       "psi_ci_sngl.csv",     row.names = FALSE)
# Reload L_median (matrix, preserving row names)
L_median <- as.matrix(
read.csv("L_median_sngl.csv",
row.names   = 1,
check.names = FALSE)
)
vars <- rownames(L_median)
# Reload df_L_ci
df_L_ci <- read.csv("L_ci_long_sngl.csv",
stringsAsFactors = FALSE,
check.names      = FALSE)
# Reload psi_median (named vector)
psi_tmp    <- read.csv("psi_median_sngl.csv", stringsAsFactors = FALSE,
check.names = FALSE)
psi_median <- psi_tmp$psi_median
names(psi_median) <- psi_tmp$variable
# Step 12 ─ Prune items via decision-tree rules
#   12.1 Identify each variable’s primary loading & its 95% CI
prim_list <- lapply(vars, function(v) {
tmp <- df_L_ci[df_L_ci$variable==v, ]
loads <- L_median[v,]; fidx <- which.max(abs(loads))
data.frame(
variable    = v,
primary_fac = names(loads)[fidx],
median_load = loads[fidx],
lower       = tmp$lower[tmp$factor==names(loads)[fidx]],
upper       = tmp$upper[tmp$factor==names(loads)[fidx]],
stringsAsFactors=FALSE
)
})
prim_df <- do.call(rbind, prim_list)
#   12.2 Apply rules: drop if CI crosses 0 AND |median|<.30; mark tentative if ≥.30
prim_df$cross_zero <- with(prim_df, lower<=0 & upper>=0)
drop1 <- prim_df$variable[prim_df$cross_zero & abs(prim_df$median_load)<.30]
tent  <- prim_df$variable[prim_df$cross_zero & abs(prim_df$median_load)>=.30]
if(length(tent)) message("Tentative (cross-zero but |load|≥.30): ", paste(tent, collapse=", "))
keep  <- setdiff(vars, drop1)
message("Dropped (cross-zero & |load|<.30): ", paste(drop1, collapse=", "))
#   12.3 Build pruned Λ and Ψ
Lambda0 <- L_median[keep, , drop=FALSE]
Psi0    <- psi_median[keep]
R_prune <- R_mixed[keep, keep]
# Step 13 ─ Prune survivors with low communality (h²<.20)
h2   <- rowSums(Lambda0^2)
drop_comm <- names(h2)[h2<0.20]
if(length(drop_comm)) message("Dropping low-h² (<.23): ", paste(drop_comm, collapse=", "))
keep_final <- setdiff(keep, drop_comm)
Lambda0    <- Lambda0[keep_final, , drop=FALSE]
Psi0       <- Psi0[keep_final]
R_prune    <- R_mixed[keep_final, keep_final]
# (… continue φ/H bootstrap, residual diagnostics, plotting …)
B         <- 1000
k         <- ncol(Lambda0)
phis_rob  <- matrix(NA_real_, B, k)
Hs_rob    <- matrix(NA_real_, B, k)
completed <- 0
attempts  <- 0
# set up cluster & progress bar
n_cores <- parallel::detectCores() - 1
cl      <- makeCluster(n_cores)
registerDoSNOW(cl)
pb       <- txtProgressBar(max = B, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts     <- list(progress = progress)
# parallel bootstrap + compute φ and H
res <- foreach(b = 1:B,
.combine    = rbind,
.packages   = c("psych","polycor"),
.options.snow = opts) %dopar% {
repeat {
# a) bootstrap sample of 'keep' cols
samp_idx <- sample(nrow(df_mix2_clean), replace = TRUE)
samp     <- df_mix2_clean[samp_idx, keep_final, drop = FALSE]
# b) mixed correlation
Rb <- tryCatch(
hetcor(samp, use = "pairwise.complete.obs")$correlations,
error = function(e) NULL
)
if (is.null(Rb) || any(is.na(Rb))) next
# c) EFA
fa_b <- tryCatch(
fa(Rb,
nfactors = k,
fm       = "minres",
rotate   = "geominQ",
n.obs    = nrow(samp)
),
error = function(e) NULL
)
if (is.null(fa_b)) next
# d) compute φ and H
Lb    <- fa_b$loadings
phi_b <- diag(factor.congruence(Lambda0, Lb))
uniqs <- 1 - rowSums(Lb[]^2)
H_b   <- vapply(seq_len(k), function(j) {
num <- sum(Lb[, j])^2
num / (num + sum(uniqs))
}, numeric(1))
# success — return a single row of length 2*k
return(c(phi_b, H_b))
}
}
# tear down
close(pb)
stopCluster(cl)
# 5. unpack results
phis_rob <- res[,        1:k,    drop = FALSE]
Hs_rob   <- res[, (k+1):(2*k),    drop = FALSE]
# 6. summarize
phi_mean <- colMeans(phis_rob)
H_mean   <- colMeans(Hs_rob)
cat(sprintf("Finished %d valid bootstraps\n", nrow(phis_rob)))
cat("Robust mean Tucker's φ: ", phi_mean, "\n")
cat("Robust mean Hancock's H:",  H_mean,   "\n")
k     <- k_MAP  # choose k
# Step 10 ─ Bootstrap robust MINRES+oblimin to get loadings & uniquenesses
p <- ncol(df_mix2_clean)
B <- 1000
n_cores <- parallel::detectCores() - 1
cl <- makeCluster(n_cores); registerDoSNOW(cl)
pb <- txtProgressBar(max=B, style=3)
opts <- list(progress = function(n) setTxtProgressBar(pb, n))
boot_load <- foreach(b=1:B, .combine=rbind,
.packages=c("psych","polycor"),
.options.snow=opts) %dopar% {
repeat {
samp <- df_mix2_clean[sample(nrow(df_mix2_clean), replace=TRUE), ]
Rb   <- tryCatch(hetcor(samp)$correlations, error=function(e) NULL)
if(is.null(Rb) || any(is.na(Rb))) next
fa_b <- tryCatch(fa(Rb, nfactors=k, fm="minres", rotate="geominQ", n.obs=nrow(samp)),
error=function(e) NULL)
if(is.null(fa_b)) next
return(c(as.vector(fa_b$loadings[]), fa_b$uniquenesses))
}
}
close(pb); stopCluster(cl)
# Step 11 ─ Summarize bootstrap: medians & 95% CIs
lambda_boot <- boot_load[, 1:(p*k)]
psi_boot    <- boot_load[, (p*k+1):(p*k+p)]
L_median    <- matrix(apply(lambda_boot, 2, median), nrow=p, ncol=k)
L_ci        <- apply(lambda_boot, 2, quantile, c(.025,.975))
psi_median  <- apply(psi_boot, 2, median)
# name dimensions
vars <- colnames(df_mix2_clean)
rownames(L_median) <- vars
colnames(L_median) <- paste0("F", 1:k)
names(psi_median)  <- vars
# reshape CIs
ci_arr     <- array(L_ci, dim=c(2,p,k))
df_L_ci    <- data.frame(
variable = rep(vars, each=k),
factor   = rep(colnames(L_median), times=p),
lower    = as.vector(ci_arr[1,,]),
upper    = as.vector(ci_arr[2,,]),
stringsAsFactors=FALSE
)
# build df_psi_ci for the uniqueness‐CI
psi_ci   <- apply(psi_boot, 2, quantile, c(.025, .975))
df_psi_ci <- data.frame(
variable = vars,
lower    = psi_ci[1, ],
upper    = psi_ci[2, ],
stringsAsFactors = FALSE
)
# build df_psi_ci for the uniqueness‐CI
psi_ci   <- apply(psi_boot, 2, quantile, c(.025, .975))
df_psi_ci <- data.frame(
variable = vars,
lower    = psi_ci[1, ],
upper    = psi_ci[2, ],
stringsAsFactors = FALSE
)
# Write medians and CIs to CSV
write.csv(L_median,        "L_median_sngl.csv",   row.names = TRUE)
write.csv(df_L_ci,         "L_ci_long_sngl.csv",  row.names = FALSE)
write.csv(data.frame(variable = vars,
psi_median = psi_median),
"psi_median_sngl.csv", row.names = FALSE)
write.csv(df_psi_ci,       "psi_ci_sngl.csv",     row.names = FALSE)
# Reload L_median (matrix, preserving row names)
L_median <- as.matrix(
read.csv("L_median_sngl.csv",
row.names   = 1,
check.names = FALSE)
)
vars <- rownames(L_median)
# Reload df_L_ci
df_L_ci <- read.csv("L_ci_long_sngl.csv",
stringsAsFactors = FALSE,
check.names      = FALSE)
# Reload psi_median (named vector)
psi_tmp    <- read.csv("psi_median_sngl.csv", stringsAsFactors = FALSE,
check.names = FALSE)
psi_median <- psi_tmp$psi_median
names(psi_median) <- psi_tmp$variable
# Step 12 ─ Prune items via decision-tree rules
#   12.1 Identify each variable’s primary loading & its 95% CI
prim_list <- lapply(vars, function(v) {
tmp <- df_L_ci[df_L_ci$variable==v, ]
loads <- L_median[v,]; fidx <- which.max(abs(loads))
data.frame(
variable    = v,
primary_fac = names(loads)[fidx],
median_load = loads[fidx],
lower       = tmp$lower[tmp$factor==names(loads)[fidx]],
upper       = tmp$upper[tmp$factor==names(loads)[fidx]],
stringsAsFactors=FALSE
)
})
prim_df <- do.call(rbind, prim_list)
#   12.2 Apply rules: drop if CI crosses 0 AND |median|<.30; mark tentative if ≥.30
prim_df$cross_zero <- with(prim_df, lower<=0 & upper>=0)
drop1 <- prim_df$variable[prim_df$cross_zero & abs(prim_df$median_load)<.30]
tent  <- prim_df$variable[prim_df$cross_zero & abs(prim_df$median_load)>=.30]
if(length(tent)) message("Tentative (cross-zero but |load|≥.30): ", paste(tent, collapse=", "))
keep  <- setdiff(vars, drop1)
message("Dropped (cross-zero & |load|<.30): ", paste(drop1, collapse=", "))
#   12.3 Build pruned Λ and Ψ
Lambda0 <- L_median[keep, , drop=FALSE]
Psi0    <- psi_median[keep]
#   12.4 Zero‐out trivial secondaries (<.15)
for(i in seq_len(nrow(Lambda0))) {
row <- Lambda0[i,]; idx <- order(abs(row), decreasing=TRUE)
sec <- idx[2]
if(abs(row[sec])<.15) Lambda0[i,sec] <- 0
}
R_prune <- R_mixed[keep, keep]
# Step 13 ─ Prune survivors with low communality (h²<.20)
h2   <- rowSums(Lambda0^2)
drop_comm <- names(h2)[h2<0.30]
if(length(drop_comm)) message("Dropping low-h² (<.23): ", paste(drop_comm, collapse=", "))
keep_final <- setdiff(keep, drop_comm)
Lambda0    <- Lambda0[keep_final, , drop=FALSE]
Psi0       <- Psi0[keep_final]
R_prune    <- R_mixed[keep_final, keep_final]
# (… continue φ/H bootstrap, residual diagnostics, plotting …)
B         <- 1000
k         <- ncol(Lambda0)
phis_rob  <- matrix(NA_real_, B, k)
Hs_rob    <- matrix(NA_real_, B, k)
completed <- 0
attempts  <- 0
# set up cluster & progress bar
n_cores <- parallel::detectCores() - 1
cl      <- makeCluster(n_cores)
registerDoSNOW(cl)
pb       <- txtProgressBar(max = B, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts     <- list(progress = progress)
# parallel bootstrap + compute φ and H
res <- foreach(b = 1:B,
.combine    = rbind,
.packages   = c("psych","polycor"),
.options.snow = opts) %dopar% {
repeat {
# a) bootstrap sample of 'keep' cols
samp_idx <- sample(nrow(df_mix2_clean), replace = TRUE)
samp     <- df_mix2_clean[samp_idx, keep_final, drop = FALSE]
# b) mixed correlation
Rb <- tryCatch(
hetcor(samp, use = "pairwise.complete.obs")$correlations,
error = function(e) NULL
)
if (is.null(Rb) || any(is.na(Rb))) next
# c) EFA
fa_b <- tryCatch(
fa(Rb,
nfactors = k,
fm       = "minres",
rotate   = "geominQ",
n.obs    = nrow(samp)
),
error = function(e) NULL
)
if (is.null(fa_b)) next
# d) compute φ and H
Lb    <- fa_b$loadings
phi_b <- diag(factor.congruence(Lambda0, Lb))
uniqs <- 1 - rowSums(Lb[]^2)
H_b   <- vapply(seq_len(k), function(j) {
num <- sum(Lb[, j])^2
num / (num + sum(uniqs))
}, numeric(1))
# success — return a single row of length 2*k
return(c(phi_b, H_b))
}
}
# tear down
close(pb)
stopCluster(cl)
# 5. unpack results
phis_rob <- res[,        1:k,    drop = FALSE]
Hs_rob   <- res[, (k+1):(2*k),    drop = FALSE]
# 6. summarize
phi_mean <- colMeans(phis_rob)
H_mean   <- colMeans(Hs_rob)
cat(sprintf("Finished %d valid bootstraps\n", nrow(phis_rob)))
cat("Robust mean Tucker's φ: ", phi_mean, "\n")
cat("Robust mean Hancock's H:",  H_mean,   "\n")
# Reload L_median (matrix, preserving row names)
L_median <- as.matrix(
read.csv("L_median_sngl.csv",
row.names   = 1,
check.names = FALSE)
)
vars <- rownames(L_median)
# Reload df_L_ci
df_L_ci <- read.csv("L_ci_long_sngl.csv",
stringsAsFactors = FALSE,
check.names      = FALSE)
# Reload psi_median (named vector)
psi_tmp    <- read.csv("psi_median_sngl.csv", stringsAsFactors = FALSE,
check.names = FALSE)
psi_median <- psi_tmp$psi_median
names(psi_median) <- psi_tmp$variable
# Step 12 ─ Prune items via decision-tree rules
#   12.1 Identify each variable’s primary loading & its 95% CI
prim_list <- lapply(vars, function(v) {
tmp <- df_L_ci[df_L_ci$variable==v, ]
loads <- L_median[v,]; fidx <- which.max(abs(loads))
data.frame(
variable    = v,
primary_fac = names(loads)[fidx],
median_load = loads[fidx],
lower       = tmp$lower[tmp$factor==names(loads)[fidx]],
upper       = tmp$upper[tmp$factor==names(loads)[fidx]],
stringsAsFactors=FALSE
)
})
prim_df <- do.call(rbind, prim_list)
#   12.2 Apply rules: drop if CI crosses 0 AND |median|<.30; mark tentative if ≥.30
prim_df$cross_zero <- with(prim_df, lower<=0 & upper>=0)
drop1 <- prim_df$variable[prim_df$cross_zero & abs(prim_df$median_load)<.30]
tent  <- prim_df$variable[prim_df$cross_zero & abs(prim_df$median_load)>=.30]
if(length(tent)) message("Tentative (cross-zero but |load|≥.30): ", paste(tent, collapse=", "))
keep  <- setdiff(vars, drop1)
message("Dropped (cross-zero & |load|<.30): ", paste(drop1, collapse=", "))
#   12.3 Build pruned Λ and Ψ
Lambda0 <- L_median[keep, , drop=FALSE]
Psi0    <- psi_median[keep]
R_prune <- R_mixed[keep, keep]
# Step 13 ─ Prune survivors with low communality (h²<.20)
h2   <- rowSums(Lambda0^2)
drop_comm <- names(h2)[h2<0.25]
if(length(drop_comm)) message("Dropping low-h² (<.23): ", paste(drop_comm, collapse=", "))
keep_final <- setdiff(keep, drop_comm)
Lambda0    <- Lambda0[keep_final, , drop=FALSE]
Psi0       <- Psi0[keep_final]
R_prune    <- R_mixed[keep_final, keep_final]
# (… continue φ/H bootstrap, residual diagnostics, plotting …)
B         <- 1000
k         <- ncol(Lambda0)
phis_rob  <- matrix(NA_real_, B, k)
Hs_rob    <- matrix(NA_real_, B, k)
completed <- 0
attempts  <- 0
# set up cluster & progress bar
n_cores <- parallel::detectCores() - 1
cl      <- makeCluster(n_cores)
registerDoSNOW(cl)
pb       <- txtProgressBar(max = B, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts     <- list(progress = progress)
# parallel bootstrap + compute φ and H
res <- foreach(b = 1:B,
.combine    = rbind,
.packages   = c("psych","polycor"),
.options.snow = opts) %dopar% {
repeat {
# a) bootstrap sample of 'keep' cols
samp_idx <- sample(nrow(df_mix2_clean), replace = TRUE)
samp     <- df_mix2_clean[samp_idx, keep_final, drop = FALSE]
# b) mixed correlation
Rb <- tryCatch(
hetcor(samp, use = "pairwise.complete.obs")$correlations,
error = function(e) NULL
)
if (is.null(Rb) || any(is.na(Rb))) next
# c) EFA
fa_b <- tryCatch(
fa(Rb,
nfactors = k,
fm       = "minres",
rotate   = "geominQ",
n.obs    = nrow(samp)
),
error = function(e) NULL
)
if (is.null(fa_b)) next
# d) compute φ and H
Lb    <- fa_b$loadings
phi_b <- diag(factor.congruence(Lambda0, Lb))
uniqs <- 1 - rowSums(Lb[]^2)
H_b   <- vapply(seq_len(k), function(j) {
num <- sum(Lb[, j])^2
num / (num + sum(uniqs))
}, numeric(1))
# success — return a single row of length 2*k
return(c(phi_b, H_b))
}
}
# tear down
close(pb)
stopCluster(cl)
# 5. unpack results
phis_rob <- res[,        1:k,    drop = FALSE]
Hs_rob   <- res[, (k+1):(2*k),    drop = FALSE]
# 6. summarize
phi_mean <- colMeans(phis_rob)
H_mean   <- colMeans(Hs_rob)
cat(sprintf("Finished %d valid bootstraps\n", nrow(phis_rob)))
cat("Robust mean Tucker's φ: ", phi_mean, "\n")
cat("Robust mean Hancock's H:",  H_mean,   "\n")
