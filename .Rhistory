small_nom    <- names(nom_levels[nom_levels < 7])
df_nom_small <- df_nom[, small_nom, drop = FALSE]
# Convert each to factor (if not already) and then to dummies
library(fastDummies)  # or use model.matrix()
df_dummies <- dummy_cols(
df_nom_small,
select_columns = small_nom,
remove_selected_columns = TRUE,
remove_first_dummy = TRUE
)
# 16.6 Fit the exploratory GAM
library(mgcv)
# sanitize all names
names(df_dummies)    <- make.names(names(df_dummies))
colnames(bart_scores) <- make.names(colnames(bart_scores))
# then rebuild df_gam and formula as before
df_gam <- bind_cols(
data.frame(prod = y_prod),
as.data.frame(bart_scores),
df_dummies
)
# now your smooth + dummy terms are already safe:
smooths     <- paste0("s(", make.names(paste0("F", 1:k)), ", bs = \"tp\")", collapse = " + ")
dummy_terms <- paste(names(df_dummies), collapse = " + ")
fml         <- as.formula(paste("prod ~", smooths, if (dummy_terms != "") paste("+", dummy_terms)))
# 16.7. Fit GAM with REML and automatic smooth selection
gam_fit <- gam(
formula = fml,
data    = df_gam,
family  = gaussian(),
method  = "REML",
select  = TRUE
)
# 16.8. Inspect results
summary(gam_fit)
# Plot factor smooths
par(mfrow = c(ceiling(k/2), 2), mar = c(4,4,2,1))
plot(gam_fit, shade = TRUE, pages = 1)
set.seed(2025)
# 1. Load data
(df <- read_excel("nepal_dataframe_FA.xlsx"))
y_prod <- df$Q0__AGR_PROD__continuous
df <- df %>% select(-Q0__AGR_PROD__continuous,
-Q0__sustainable_livelihood_score__continuous)
# 3. Split into continuous / ordinal / binary / nominal
(types  <- str_split(names(df), "__", simplify = TRUE)[,3])
types[types == "binary_nominal"] <- "nominal"
df_cont <- df[, types == "continuous", drop = FALSE]
df_ord  <- df[, types == "ordinal",    drop = FALSE]
df_bin  <- df[, types == "binary",     drop = FALSE]
df_nom  <- df[, types == "nominal",    drop = FALSE]
# 4. Handle high-cardinality nominals via categorical PCA
(nom_levels <- sapply(df_nom, function(x) length(unique(x))))
high_nom   <- names(nom_levels[nom_levels > 6])
if (length(high_nom) > 0) {
# Convert to factors and run princals
df_nom[high_nom] <- lapply(df_nom[high_nom], factor)
m      <- length(high_nom)
pc_out <- princals(
data    = as.data.frame(df_nom[high_nom]),
ndim    = m,
levels  = rep("nominal", m),
verbose = FALSE
)
# Sanity-check object scores
if (!"objectscores" %in% names(pc_out)) {
stop("princals() did not return 'objectscores'!")
}
dims <- dim(pc_out$objectscores)
if (!all(dims == c(nrow(df_nom), m))) {
stop(sprintf(
"Unexpected objectscores dimensions: got %dx%d but expected %dx%d",
dims[1], dims[2], nrow(df_nom), m
))
}
# Extract and append quantified scores, drop originals
df_quant <- as.data.frame(pc_out$objectscores)
names(df_quant) <- paste0(high_nom, "_quant")
df_cont <- bind_cols(df_cont, df_quant)
df      <- df %>% select(-all_of(high_nom))
df_nom  <- df_nom[, setdiff(names(df_nom), high_nom), drop = FALSE]
}
# 5. Identify and drop near-zero variance _quant columns
quant_cols <- grep("_quant$", names(df_cont), value = TRUE)
variances  <- sapply(df_cont[quant_cols], var, na.rm = TRUE)
zero_var   <- names(variances)[variances < 1e-6]
if (length(zero_var) > 0) {
warning("Dropping near-constant quant cols: ", paste(zero_var, collapse = ", "))
df_cont   <- df_cont %>% select(-all_of(zero_var))
quant_cols <- setdiff(quant_cols, zero_var)
}
# 6. Standardize remaining quant columns
if (length(quant_cols) > 0) {
df_cont[quant_cols] <- lapply(df_cont[quant_cols], scale)
}
# 7. Factor df_ord and df_bin as ordered
(df_ord_factored <- df_ord %>% mutate(across(everything(), ordered)))
(df_bin_factored <- df_bin %>% mutate(across(everything(), ordered)))
# 8. Rebuild mixed-type data frame and clean
(df_mix2 <- bind_cols(df_cont, df_ord_factored, df_bin_factored))
(df_mix2_clean <- df_mix2[, colSums(is.na(df_mix2)) == 0])
# Debug: column classes and drop unsupported
cat(">>> DEBUG: column classes:\n")
print(sapply(df_mix2_clean, class))
allowed <- function(x) {
inherits(x, c("numeric","integer","factor","ordered","logical","character"))
}
good_cols <- vapply(df_mix2_clean, allowed, logical(1))
bad_cols  <- names(good_cols)[!good_cols]
if (length(bad_cols) > 0) {
cat(">>> WARNING: dropping unsupported columns:\n")
print(bad_cols)
df_mix2_clean <- df_mix2_clean[, good_cols]
}
cat(">>> POST‐CLEAN: column classes:\n")
print(sapply(df_mix2_clean, class))
(df_mix2_clean <- as.data.frame(df_mix2_clean))
cat("Post‐conversion class: ", class(df_mix2_clean), "\n")
# 9. Compute heterogeneous correlation matrix and compare eigenvalues
het_out   <- hetcor(df_mix2_clean, use = "pairwise.complete.obs")
R_mixed   <- het_out$correlations
stopifnot(!any(is.na(R_mixed)))
ev_raw <- eigen(hetcor(df_mix2_clean)$correlations)$values
ev_adj <- eigen(R_mixed)$values
plot(ev_raw, ev_adj, main = "Eigenvalue comparison")
# 10. Parallel analysis, MAP, and choose k
pa_out  <- fa.parallel(R_mixed, n.obs = nrow(df_mix2_clean),
fm = "minres", fa = "fa",
n.iter = 500, quant = .95,
cor = "cor", use = "pairwise",
plot = FALSE)
k_PA    <- pa_out$nfact
vss_out <- VSS(R_mixed, n = ncol(R_mixed),
fm = "minres", n.obs = nrow(df_mix2_clean), plot = FALSE)
k_MAP   <- which.min(vss_out$map)
k       <- k_MAP  # choose as desired
k       <- k_MAP  # choose as desired
# 11. Initial MINRES + oblimin
efa_init <- fa(R_mixed, nfactors = k,
fm = "minres", rotate = "oblimin",
n.obs = nrow(df_mix2_clean))
# 12. Prune low-communality items and re-run EFA
h2      <- efa_init$communality
keep    <- names(h2)[h2 >= .25]
R_prune <- R_mixed[keep, keep]
efa0    <- fa(R_prune, nfactors = k,
fm = "minres", rotate = "oblimin",
n.obs = nrow(df_mix2_clean))
Lambda0 <- efa0$loadings
Psi0    <- efa0$uniquenesses
Lambda0
# 13. Bootstrap for Tucker’s φ & Hancock’s H
B         <- 1000
phis      <- matrix(NA, B, k)
Hs        <- matrix(NA, B, k)
completed <- 0
while (completed < B) {
samp    <- df_mix2_clean[sample(nrow(df_mix2_clean), replace = TRUE), keep]
het_b   <- hetcor(as.data.frame(samp), use = "pairwise.complete.obs")
Rb      <- het_b$correlations
fa_b    <- tryCatch(
fa(Rb, nfactors = k, fm = "minres", rotate = "oblimin", n.obs = nrow(samp)),
error = function(e) NULL
)
if (is.null(fa_b)) next
completed       <- completed + 1
Lb              <- fa_b$loadings
psib            <- fa_b$uniquenesses
phis[completed, ] <- diag(factor.congruence(Lambda0, Lb))
Hs[completed, ]   <- vapply(seq_len(k), function(j) {
sum(Lb[, j])^2 / (sum(Lb[, j])^2 + sum(psib))
}, numeric(1))
}
# 14. Report stability
cat("Mean Tucker’s φ: ", colMeans(phis), "\n")
cat("Mean Hancock’s H: ", colMeans(Hs), "\n")
# 15.1 Communalities (h²) from Λ
h2 <- rowSums(Lambda0^2)
cat("Mean communality (h²):", mean(h2), "\n")
print(head(data.frame(variable = names(h2), communality = h2), 10))
# 1) Create the diagonal matrix from Psi0
Psi_mat <- diag(Psi0)
# 1) Create the diagonal matrix from Psi0
Psi_mat <- diag(Psi0)
# 2) Assign row- and column names
rownames(Psi_mat) <- names(Psi0)
colnames(Psi_mat) <- names(Psi0)
# 15.3 Compute residual matrix: R_resid = R_prune − ΛΛᵀ − Ψ
resid_mat <- R_prune - (Lambda0 %*% t(Lambda0)) - Psi_mat
# 15.3 Compute residual matrix: R_resid = R_prune − ΛΛᵀ − Ψ
resid_mat <- R_prune - (Lambda0 %*% t(Lambda0)) - Psi_mat
# 15.4 Overall fit: RMSR on the off-diagonals
off_diag_vals <- resid_mat[lower.tri(resid_mat)]
RMSR <- sqrt(mean(off_diag_vals^2))
cat("RMSR =", round(RMSR, 4), "\n")
# 15.5 Identify any |residual| > .10
off_idx <- which(abs(resid_mat) > 0.10, arr.ind = TRUE)
if (nrow(off_idx) > 0) {
offenders <- data.frame(
var1     = rownames(resid_mat)[off_idx[,1]],
var2     = colnames(resid_mat)[off_idx[,2]],
residual = resid_mat[off_idx]
)
cat("Residuals exceeding |.10|:\n")
print(offenders)
} else {
cat("No residuals exceed |0.10|.\n")
}
# 16.1. Extract the raw data for your 'keep' items
raw_items <- df_mix2_clean[, keep, drop = FALSE]
# 16.2. Convert any factor/ordered/character columns to numeric
raw_items_num <- lapply(raw_items, function(col) {
# if already numeric/integer, leave it
if (is.numeric(col)) return(col)
# if ordered or factor, map levels -> integers
if (is.ordered(col) || is.factor(col)) return(as.numeric(as.character(col)))
# if character, try numeric parse
if (is.character(col))  return(as.numeric(col))
stop("Unsupported column type in raw_items: ", class(col)[1])
})
raw_items_num <- as.data.frame(raw_items_num)
# 16.3. Now compute Bartlett factor scores
bart_scores <- factor.scores(
x      = raw_items_num,
f      = efa0,
method = "Bartlett"
)$scores
# 16.4. Label them F1…Fk
colnames(bart_scores) <- paste0("F", seq_len(ncol(bart_scores)))
# 16.5. Build nominal dummies for small-cardinality nominals
# df_nom still holds all nominal variables not handled by princals()
nom_levels   <- sapply(df_nom, function(x) length(unique(x)))
small_nom    <- names(nom_levels[nom_levels < 7])
df_nom_small <- df_nom[, small_nom, drop = FALSE]
# Convert each to factor (if not already) and then to dummies
library(fastDummies)  # or use model.matrix()
df_dummies <- dummy_cols(
df_nom_small,
select_columns = small_nom,
remove_selected_columns = TRUE,
remove_first_dummy = TRUE
)
# 16.6 Fit the exploratory GAM
library(mgcv)
# sanitize all names
names(df_dummies)    <- make.names(names(df_dummies))
colnames(bart_scores) <- make.names(colnames(bart_scores))
# then rebuild df_gam and formula as before
df_gam <- bind_cols(
data.frame(prod = y_prod),
as.data.frame(bart_scores),
df_dummies
)
# now your smooth + dummy terms are already safe:
smooths     <- paste0("s(", make.names(paste0("F", 1:k)), ", bs = \"tp\")", collapse = " + ")
dummy_terms <- paste(names(df_dummies), collapse = " + ")
fml         <- as.formula(paste("prod ~", smooths, if (dummy_terms != "") paste("+", dummy_terms)))
# 16.7. Fit GAM with REML and automatic smooth selection
gam_fit <- gam(
formula = fml,
data    = df_gam,
family  = gaussian(),
method  = "REML",
select  = TRUE
)
# 16.8. Inspect results
summary(gam_fit)
# Plot factor smooths
par(mfrow = c(ceiling(k/2), 2), mar = c(4,4,2,1))
plot(gam_fit, shade = TRUE, pages = 1)
Lambda0
plot(gam_fit, shade = TRUE, pages = 1)
plot(gam_fit, shade = TRUE, pages = 3)
plot(gam_fit, shade = TRUE, pages = 1)
# Load required packages
library(dplyr)       # data wrangling
library(EFAtools)    # VSS()
library(boot)        # bootstrap()
library(Gifi)        # princals()
library(lavaan)      # sem()
library(mgcv)        # gam()
library(polycor)     # hetcor()
library(psych)       # mixedCor(), fa.*, factor.congruence()
library(readxl)      # read_excel()
library(stringr)     # str_split()
set.seed(2025)
# 1. Load data
(df <- read_excel("senegal_dataframe_FA.xlsx"))
y_prod <- df$Q0__AGR_PROD__continuous
df <- df %>% select(-Q0__AGR_PROD__continuous,
-Q0__sustainable_livelihood_score__continuous)
# 3. Split into continuous / ordinal / binary / nominal
(types  <- str_split(names(df), "__", simplify = TRUE)[,3])
types[types == "binary_nominal"] <- "nominal"
df_cont <- df[, types == "continuous", drop = FALSE]
df_ord  <- df[, types == "ordinal",    drop = FALSE]
df_bin  <- df[, types == "binary",     drop = FALSE]
df_nom  <- df[, types == "nominal",    drop = FALSE]
# 4. Handle high-cardinality nominals via categorical PCA
(nom_levels <- sapply(df_nom, function(x) length(unique(x))))
high_nom   <- names(nom_levels[nom_levels > 6])
if (length(high_nom) > 0) {
# Convert to factors and run princals
df_nom[high_nom] <- lapply(df_nom[high_nom], factor)
m      <- length(high_nom)
pc_out <- princals(
data    = as.data.frame(df_nom[high_nom]),
ndim    = m,
levels  = rep("nominal", m),
verbose = FALSE
)
# Sanity-check object scores
if (!"objectscores" %in% names(pc_out)) {
stop("princals() did not return 'objectscores'!")
}
dims <- dim(pc_out$objectscores)
if (!all(dims == c(nrow(df_nom), m))) {
stop(sprintf(
"Unexpected objectscores dimensions: got %dx%d but expected %dx%d",
dims[1], dims[2], nrow(df_nom), m
))
}
# Extract and append quantified scores, drop originals
df_quant <- as.data.frame(pc_out$objectscores)
names(df_quant) <- paste0(high_nom, "_quant")
df_cont <- bind_cols(df_cont, df_quant)
df      <- df %>% select(-all_of(high_nom))
df_nom  <- df_nom[, setdiff(names(df_nom), high_nom), drop = FALSE]
}
# 5. Identify and drop near-zero variance _quant columns
quant_cols <- grep("_quant$", names(df_cont), value = TRUE)
variances  <- sapply(df_cont[quant_cols], var, na.rm = TRUE)
zero_var   <- names(variances)[variances < 1e-6]
if (length(zero_var) > 0) {
warning("Dropping near-constant quant cols: ", paste(zero_var, collapse = ", "))
df_cont   <- df_cont %>% select(-all_of(zero_var))
quant_cols <- setdiff(quant_cols, zero_var)
}
# 6. Standardize remaining quant columns
if (length(quant_cols) > 0) {
df_cont[quant_cols] <- lapply(df_cont[quant_cols], scale)
}
# 7. Factor df_ord and df_bin as ordered
(df_ord_factored <- df_ord %>% mutate(across(everything(), ordered)))
(df_bin_factored <- df_bin %>% mutate(across(everything(), ordered)))
# 8. Rebuild mixed-type data frame and clean
(df_mix2 <- bind_cols(df_cont, df_ord_factored, df_bin_factored))
(df_mix2_clean <- df_mix2[, colSums(is.na(df_mix2)) == 0])
# Debug: column classes and drop unsupported
cat(">>> DEBUG: column classes:\n")
print(sapply(df_mix2_clean, class))
allowed <- function(x) {
inherits(x, c("numeric","integer","factor","ordered","logical","character"))
}
good_cols <- vapply(df_mix2_clean, allowed, logical(1))
bad_cols  <- names(good_cols)[!good_cols]
if (length(bad_cols) > 0) {
cat(">>> WARNING: dropping unsupported columns:\n")
print(bad_cols)
df_mix2_clean <- df_mix2_clean[, good_cols]
}
cat(">>> POST‐CLEAN: column classes:\n")
print(sapply(df_mix2_clean, class))
(df_mix2_clean <- as.data.frame(df_mix2_clean))
cat("Post‐conversion class: ", class(df_mix2_clean), "\n")
# 9. Compute heterogeneous correlation matrix and compare eigenvalues
het_out   <- hetcor(df_mix2_clean, use = "pairwise.complete.obs")
R_mixed   <- het_out$correlations
stopifnot(!any(is.na(R_mixed)))
ev_raw <- eigen(hetcor(df_mix2_clean)$correlations)$values
ev_adj <- eigen(R_mixed)$values
plot(ev_raw, ev_adj, main = "Eigenvalue comparison")
# 10. Parallel analysis, MAP, and choose k
pa_out  <- fa.parallel(R_mixed, n.obs = nrow(df_mix2_clean),
fm = "minres", fa = "fa",
n.iter = 500, quant = .95,
cor = "cor", use = "pairwise",
plot = FALSE)
k_PA    <- pa_out$nfact
vss_out <- VSS(R_mixed, n = ncol(R_mixed),
fm = "minres", n.obs = nrow(df_mix2_clean), plot = FALSE)
k_MAP   <- which.min(vss_out$map)
k       <- k_MAP  # choose as desired
# 11. Initial MINRES + oblimin
efa_init <- fa(R_mixed, nfactors = k,
fm = "minres", rotate = "oblimin",
n.obs = nrow(df_mix2_clean))
# 12. Prune low-communality items and re-run EFA
h2      <- efa_init$communality
keep    <- names(h2)[h2 >= .25]
R_prune <- R_mixed[keep, keep]
efa0    <- fa(R_prune, nfactors = k,
fm = "minres", rotate = "oblimin",
n.obs = nrow(df_mix2_clean))
Lambda0 <- efa0$loadings
Psi0    <- efa0$uniquenesses
Lambda0
# 13. Bootstrap for Tucker’s φ & Hancock’s H
B         <- 1000
phis      <- matrix(NA, B, k)
Hs        <- matrix(NA, B, k)
completed <- 0
while (completed < B) {
samp    <- df_mix2_clean[sample(nrow(df_mix2_clean), replace = TRUE), keep]
het_b   <- hetcor(as.data.frame(samp), use = "pairwise.complete.obs")
Rb      <- het_b$correlations
fa_b    <- tryCatch(
fa(Rb, nfactors = k, fm = "minres", rotate = "oblimin", n.obs = nrow(samp)),
error = function(e) NULL
)
if (is.null(fa_b)) next
completed       <- completed + 1
Lb              <- fa_b$loadings
psib            <- fa_b$uniquenesses
phis[completed, ] <- diag(factor.congruence(Lambda0, Lb))
Hs[completed, ]   <- vapply(seq_len(k), function(j) {
sum(Lb[, j])^2 / (sum(Lb[, j])^2 + sum(psib))
}, numeric(1))
}
# 14. Report stability
cat("Mean Tucker’s φ: ", colMeans(phis), "\n")
cat("Mean Hancock’s H: ", colMeans(Hs), "\n")
# 15.1 Communalities (h²) from Λ
h2 <- rowSums(Lambda0^2)
cat("Mean communality (h²):", mean(h2), "\n")
print(head(data.frame(variable = names(h2), communality = h2), 10))
# 1) Create the diagonal matrix from Psi0
Psi_mat <- diag(Psi0)
# 2) Assign row- and column names
rownames(Psi_mat) <- names(Psi0)
colnames(Psi_mat) <- names(Psi0)
# 15.3 Compute residual matrix: R_resid = R_prune − ΛΛᵀ − Ψ
resid_mat <- R_prune - (Lambda0 %*% t(Lambda0)) - Psi_mat
# 15.4 Overall fit: RMSR on the off-diagonals
off_diag_vals <- resid_mat[lower.tri(resid_mat)]
RMSR <- sqrt(mean(off_diag_vals^2))
cat("RMSR =", round(RMSR, 4), "\n")
# 15.5 Identify any |residual| > .10
off_idx <- which(abs(resid_mat) > 0.10, arr.ind = TRUE)
if (nrow(off_idx) > 0) {
offenders <- data.frame(
var1     = rownames(resid_mat)[off_idx[,1]],
var2     = colnames(resid_mat)[off_idx[,2]],
residual = resid_mat[off_idx]
)
cat("Residuals exceeding |.10|:\n")
print(offenders)
} else {
cat("No residuals exceed |0.10|.\n")
}
# 16.1. Extract the raw data for your 'keep' items
raw_items <- df_mix2_clean[, keep, drop = FALSE]
# 16.2. Convert any factor/ordered/character columns to numeric
raw_items_num <- lapply(raw_items, function(col) {
# if already numeric/integer, leave it
if (is.numeric(col)) return(col)
# if ordered or factor, map levels -> integers
if (is.ordered(col) || is.factor(col)) return(as.numeric(as.character(col)))
# if character, try numeric parse
if (is.character(col))  return(as.numeric(col))
stop("Unsupported column type in raw_items: ", class(col)[1])
})
raw_items_num <- as.data.frame(raw_items_num)
# 16.3. Now compute Bartlett factor scores
bart_scores <- factor.scores(
x      = raw_items_num,
f      = efa0,
method = "Bartlett"
)$scores
# 16.4. Label them F1…Fk
colnames(bart_scores) <- paste0("F", seq_len(ncol(bart_scores)))
# 16.5. Build nominal dummies for small-cardinality nominals
# df_nom still holds all nominal variables not handled by princals()
nom_levels   <- sapply(df_nom, function(x) length(unique(x)))
small_nom    <- names(nom_levels[nom_levels < 7])
df_nom_small <- df_nom[, small_nom, drop = FALSE]
# Convert each to factor (if not already) and then to dummies
library(fastDummies)  # or use model.matrix()
df_dummies <- dummy_cols(
df_nom_small,
select_columns = small_nom,
remove_selected_columns = TRUE,
remove_first_dummy = TRUE
)
# 16.6 Fit the exploratory GAM
library(mgcv)
# sanitize all names
names(df_dummies)    <- make.names(names(df_dummies))
colnames(bart_scores) <- make.names(colnames(bart_scores))
# then rebuild df_gam and formula as before
df_gam <- bind_cols(
data.frame(prod = y_prod),
as.data.frame(bart_scores),
df_dummies
)
# now your smooth + dummy terms are already safe:
smooths     <- paste0("s(", make.names(paste0("F", 1:k)), ", bs = \"tp\")", collapse = " + ")
dummy_terms <- paste(names(df_dummies), collapse = " + ")
fml         <- as.formula(paste("prod ~", smooths, if (dummy_terms != "") paste("+", dummy_terms)))
# 16.7. Fit GAM with REML and automatic smooth selection
gam_fit <- gam(
formula = fml,
data    = df_gam,
family  = gaussian(),
method  = "REML",
select  = TRUE
)
# 16.8. Inspect results
summary(gam_fit)
# Plot factor smooths
par(mfrow = c(ceiling(k/2), 2), mar = c(4,4,2,1))
plot(gam_fit, shade = TRUE, pages = 1)
