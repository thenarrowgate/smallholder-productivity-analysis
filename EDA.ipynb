{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6798a-71e9-4acd-a8e3-67bdf4ea84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import folium\n",
    "import seaborn as sns\n",
    "from helpers import parse_feature_metadata\n",
    "import osmnx as ox\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453a901",
   "metadata": {},
   "source": [
    "Please set this constant to false if you wish the cells not to plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_GRAPHS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ffd2c",
   "metadata": {},
   "source": [
    "Local paths should be correct if repository was cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ae2dd-4233-41ac-a1d7-f3605d64a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEPAL_PATH = r\"data\\raw_updated_names\\nepal_new_feature_names.xlsx\"\n",
    "SENEGAL_PATH = r\"data\\raw_updated_names\\senegal_new_feature_names.xlsx\"\n",
    "\n",
    "nepal_df = pd.read_excel(NEPAL_PATH, index_col=0)\n",
    "senegal_df = pd.read_excel(SENEGAL_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75112d0-013b-4f50-bda8-42f2bce41966",
   "metadata": {},
   "source": [
    "## 1. Initial Parsing of the Data\n",
    "This phase includes removal of features that we from the start knew we would not use or would not be beneficial, and also coerces the remaining features's values to be their declared types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd9986-4d7f-4d05-8373-ac1ca0405c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEPAL_CAT_COL_NAMES = {'Q11': 'Marital_Status',\n",
    " 'Q53': 'What_type_of_crop_is_grown_on_this_plot',\n",
    " 'Q54': 'For_vegetables_what_is_your_source_of_seeds',\n",
    " 'Q56': 'For_vegetables_do_you_use_seedlings',\n",
    " 'Q58': 'fertilizer_on_this_plot',\n",
    " 'Q63': 'What_is_the_main_use_of_produce_from_holding',\n",
    " 'Q64': 'Do_you_use_machinery_or_and_equipment_on_the_plot',\n",
    " 'Q65': 'Do_you_do_any_of_the_following',\n",
    " 'Q67': 'What_do_you_use_soil_analysis_for',\n",
    " 'Q68': 'How_do_you_conduct_soil_analysis',\n",
    " 'Q69': 'What_is_correct_for_you',\n",
    " 'Q71': 'in_the_past_12_months_from_who_did_you_receive_info_on_agriculture',\n",
    " 'Q73': 'Did_you_receive_anything_from_these_organizations',\n",
    " 'Q74': 'How_do_you_decide_to_plow',\n",
    " 'Q75': 'How_do_you_decide_to_begin_sowing',\n",
    " 'Q76': 'What_type_of_irrigation_do_you_use',\n",
    " 'Q100': 'Caste',\n",
    " 'Q105': 'main_sources_of_income',\n",
    " 'Q111': 'Generally_speaking_would_you_say_that_most_people_can_be_trusted',\n",
    " 'Q112': 'I_am_much_better_than_most_farmers_here',\n",
    " 'Q113': 'dislike_not_knowing_what_is_going_to_happen'}\n",
    "\n",
    "SENEGAL_CAT_COL_NAMES = {'Q63': 'CROP',\n",
    " 'Q64': 'Seed_source',\n",
    " 'Q65': 'Variety',\n",
    " 'Q66': 'Seedlings',\n",
    " 'Q67': 'Fertilizer',\n",
    " 'Q72': 'Sold_VEG',\n",
    " 'Q73': 'Machinery',\n",
    " 'Q74': 'Practice',\n",
    " 'Q77': 'Info_source',\n",
    " 'Q79': 'Did_you_receive_anything_from_the_specified_organizations',\n",
    " 'Q80': 'Plow_weather',\n",
    " 'Q81': 'Sow',\n",
    " 'Q82': 'Irrigation',\n",
    " 'Q88': 'family_main_sources_income',\n",
    " 'Q89': 'education_level'}\n",
    "\n",
    "def fix_column_values(series: pd.Series, meta: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given a pandas Series and its parsed metadata,\n",
    "    coerce its values to the right type:\n",
    "      - continuous, ordinal → numeric (floats or ints; bad → NaN)\n",
    "      - nominal -> categorical\n",
    "      - binary → 0/1 integers   (anything non-zero → 1, missing -> NaN)\n",
    "      - time   → pandas datetime (bad → NaT)\n",
    "    \"\"\"\n",
    "    ftype = meta[\"type\"]\n",
    "    fname = meta[\"name\"].lower()\n",
    "\n",
    "    if ftype in (\"continuous\", \"ordinal\"):\n",
    "        # numeric codes or measurements\n",
    "        return pd.to_numeric(series, errors=\"coerce\")\n",
    "    if ftype == \"nominal\":\n",
    "        return series.astype(\"category\")\n",
    "    if \"binary\" in ftype:\n",
    "        # strings that are not '0': → 1, otherwise __> 0, floats/ints: non-zero → 1, zero or NaN → 0\n",
    "        def to_binary(x):\n",
    "            if pd.isna(x):\n",
    "                return pd.NA\n",
    "            if isinstance(x, str):\n",
    "                return x != '0'\n",
    "            try:\n",
    "                return 1 if float(x) != 0 else 0\n",
    "            except:\n",
    "                return 1\n",
    "        return series.map(to_binary).astype(\"Int64\")\n",
    "\n",
    "    if ftype == \"time\":\n",
    "        # Force everything to str so we have uniform input\n",
    "        s = series.astype(str)\n",
    "        # 0) special: End_Date holds a full datetime → extract only the time\n",
    "        if \"end_date\" in fname:\n",
    "            # parses strings like \"4/27/2018 9:47:17 AM\" into Timestamps\n",
    "            dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "            # grab Python datetime.time\n",
    "            return dt.dt.time\n",
    "        # 1) true date column → datetime64\n",
    "        if \"date\" in fname:\n",
    "            return pd.to_datetime(s, format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "        # 2) survey‐length durations → timedelta64\n",
    "        if \"length\" in fname:\n",
    "            # strings like \"00:21:14\" → Timedelta\n",
    "            return pd.to_timedelta(s, errors=\"coerce\")\n",
    "\n",
    "        # 3) the two pure clock‐time columns → Python time\n",
    "        #    strings like \"11:27:52\" → Timestamp → .time()\n",
    "        parsed = pd.to_datetime(s, format=\"%H:%M:%S\", errors=\"coerce\")\n",
    "        return parsed.dt.time\n",
    "\n",
    "    # otherwise leave it alone\n",
    "    return series\n",
    "\n",
    "\n",
    "def clean_survey_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a copy of df in which:\n",
    "      - all columns named Q<digits>__… have been coerced to their declared types\n",
    "      - all other columns are left untouched (you can drop them later)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        meta = parse_feature_metadata(col)\n",
    "        if meta is None:\n",
    "            continue\n",
    "        df[col] = fix_column_values(df[col], meta)\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_non_relevant_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove everything except Q<digits> columns:\n",
    "    \"\"\"\n",
    "    keep = [col for col in df.columns if parse_feature_metadata(col) is not None]\n",
    "    return df[keep]\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def combine_dummy_columns(orig_df: pd.DataFrame, names_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combines your binary dummy columns into single categorical columns,\n",
    "    but now *preserves* the option labels.\n",
    "\n",
    "    For each (qid, type):\n",
    "      - If there's exactly one dummy → just rename it to end in `-1`.\n",
    "      - If there are many and it's one-hot (max 1 per row) → new column takes the option LABEL, or None.\n",
    "      - Otherwise (multi-select) → new column is a list of the LABELS selected (possibly empty).\n",
    "    \"\"\"\n",
    "    df = orig_df.copy()\n",
    "\n",
    "    # 1) Gather metadata & group columns by (qid, type)\n",
    "    meta_map   = {}               # col_name → {qid, qname, var_type, dummy_idx, label}\n",
    "    buckets    = defaultdict(list)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if \"binary\" not in col:\n",
    "            continue\n",
    "        m = parse_feature_metadata(col)\n",
    "        qid        = m[\"qid\"]\n",
    "        qname      = m[\"name\"]\n",
    "        var_type   = m[\"type\"]\n",
    "        dummy_idx  = m[\"dummy\"]\n",
    "        \n",
    "        subnames = qname.split(':')\n",
    "        label      = qname\n",
    "\n",
    "        meta_map[col] = {\n",
    "            \"qid\":       qid,\n",
    "            \"qname\":     qname,\n",
    "            \"var_type\":  var_type,\n",
    "            \"dummy_idx\": dummy_idx,\n",
    "            \"label\":     label\n",
    "        }\n",
    "        buckets[(qid, var_type)].append(col)\n",
    "\n",
    "    to_drop  = []\n",
    "    new_cols = {}\n",
    "\n",
    "    # 2) Process each question group\n",
    "    for (qid, var_type), cols in buckets.items():\n",
    "        # sort by dummy_idx so labels stay in the right order\n",
    "        cols_sorted   = sorted(cols, key=lambda c: meta_map[c][\"dummy_idx\"])\n",
    "        labels_sorted = [meta_map[c][\"label\"] for c in cols_sorted]\n",
    "        arr           = df[cols_sorted].fillna(0).astype(int).values\n",
    "\n",
    "        # A) Single-dummy → just rename it to ...-1\n",
    "        if len(cols_sorted) == 1:\n",
    "            old = cols_sorted[0]\n",
    "            new = f\"{qid}__{meta_map[old]['qname']}__binary__1\"\n",
    "            df = df.rename(columns={old: new})\n",
    "            continue\n",
    "        \n",
    "\n",
    "        new_type = \"nominal\" if var_type.endswith(\"nominal\") else \"ordinal\"\n",
    "        qname = names_dict[qid]\n",
    "        new_name = f\"{qid}__{qname}__{new_type}\"\n",
    "        \n",
    "\n",
    "        # B) One-hot? (no row has more than one “1”)\n",
    "        if arr.sum(axis=1).max() <= 1:\n",
    "            cat = []\n",
    "            for row in arr:\n",
    "                if row.sum() == 0:\n",
    "                    cat.append(None)\n",
    "                else:\n",
    "                    cat.append(labels_sorted[row.argmax()])\n",
    "            \n",
    "            new_cols[new_name] = cat\n",
    "\n",
    "        # C) Multi-select\n",
    "        else:\n",
    "            multi = []\n",
    "            for row in arr:\n",
    "                # collect all the labels whose dummy==1\n",
    "                sel = [labels_sorted[i] for i, v in enumerate(row) if v == 1]\n",
    "                multi.append(tuple(sel))\n",
    "            new_cols[new_name] = multi\n",
    "\n",
    "        to_drop.extend(cols_sorted)\n",
    "\n",
    "    # 3) Drop old dummies & add the new columns\n",
    "    if to_drop:\n",
    "        df = df.drop(columns=to_drop)\n",
    "        df = pd.concat([df, pd.DataFrame(new_cols, index=df.index)], axis=1)\n",
    "\n",
    "    return df, new_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e67bc-2528-4ad9-b223-a92f9e7df7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping non Q#... columns\n",
    "nepal_df = drop_non_relevant_columns(nepal_df)\n",
    "senegal_df = drop_non_relevant_columns(senegal_df)\n",
    "\n",
    "# initial clean\n",
    "nepal_df = clean_survey_dataframe(nepal_df)\n",
    "senegal_df = clean_survey_dataframe(senegal_df)\n",
    "\n",
    "# combine and remove dummy columns\n",
    "nepal_df, npl_new_cols = combine_dummy_columns(nepal_df, NEPAL_CAT_COL_NAMES)\n",
    "senegal_df, sng_new_cols = combine_dummy_columns(senegal_df, SENEGAL_CAT_COL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21276580-c1c8-416a-8b09-ed65896e4f6e",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bdcf07-96ed-4c8f-9ff2-9f2a6401f884",
   "metadata": {},
   "source": [
    "## Analyze Datetime columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8ccc3",
   "metadata": {},
   "source": [
    "Create mew datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcfa54-48ab-45d1-9878-094a95bdaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nepal_df['Q2__SurveyedDate__time'] = pd.to_datetime(nepal_df['Q2__SurveyedDate__time'], errors='coerce')\n",
    "\n",
    "def create_datetime_col(df):\n",
    "    # ensure your date column is datetime64\n",
    "    df['Q2__SurveyedDate__time'] = pd.to_datetime(\n",
    "        df['Q2__SurveyedDate__time'], format=\"%Y-%m-%d\", errors='coerce'\n",
    "    )\n",
    "\n",
    "    # 1) build an all‐string “HH:MM:SS” series\n",
    "    time_str = df['Q2__SurveyedTime__time'].astype(str).str.extract(r'(\\d{2}:\\d{2}:\\d{2})')[0]\n",
    "\n",
    "    # 2) combine\n",
    "    dt_strings = df['Q2__SurveyedDate__time'].dt.strftime('%Y-%m-%d') + ' ' + time_str\n",
    "\n",
    "    # 3) parse into real timestamps\n",
    "    df['Q2__Surveyed_Date_Time__time'] = pd.to_datetime(dt_strings, errors='coerce')\n",
    "\n",
    "    print(df['Q2__Surveyed_Date_Time__time'].dtype)  # should be datetime64[ns]\n",
    "    \n",
    "create_datetime_col(nepal_df)\n",
    "create_datetime_col(senegal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66127322",
   "metadata": {},
   "source": [
    "Removed old one since already have DateTime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa142c-1801-4e67-b489-36231346af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nepal_df = nepal_df.drop(['Q2__SurveyedDate__time', 'Q2__SurveyedTime__time'], axis=1)\n",
    "senegal_df = senegal_df.drop(['Q2__SurveyedDate__time', 'Q2__SurveyedTime__time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd1d5c",
   "metadata": {},
   "source": [
    "Define function to analyze new datetime columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36661c-fad0-4194-aa29-352e7bdd0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_datetime(df, col):\n",
    "    \"\"\"\n",
    "    Perform univariate analysis on a datetime64 column:\n",
    "      - prints basic stats (min, max, missing)\n",
    "      - plots counts per day, month, weekday, and hour-of-day\n",
    "    \"\"\"\n",
    "    # Basic info\n",
    "    series = df[col]\n",
    "    if not series.dtype.name.startswith('datetime'):\n",
    "        raise ValueError(f\"Column {col!r} is not datetime dtype.\")\n",
    "    \n",
    "    print(f\"\\n--- ANALYSIS OF {col.upper()} ---\")\n",
    "    print(f\"Type: {series.dtype}\")\n",
    "    print(f\"Missing: {series.isna().sum()} / {len(series)}\")\n",
    "    print(f\"Range: {series.min()} → {series.max()}\")\n",
    "    \n",
    "    # Extract time features\n",
    "    df_temp = df.dropna(subset=[col]).copy()\n",
    "    df_temp['date']    = df_temp[col].dt.date\n",
    "    df_temp['month']   = df_temp[col].dt.to_period('M').astype(str)\n",
    "    df_temp['weekday'] = df_temp[col].dt.day_name()\n",
    "    df_temp['hour']    = df_temp[col].dt.hour\n",
    "    \n",
    "    # 1. Counts per day\n",
    "    daily = df_temp.groupby('date').size()\n",
    "    plt.figure()\n",
    "    daily.plot(kind='bar')\n",
    "    plt.title(f\"Number of Records per Day ({col})\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Counts by month\n",
    "    monthly = df_temp.groupby('month').size().sort_index()\n",
    "    plt.figure()\n",
    "    monthly.plot(kind='bar')\n",
    "    plt.title(f\"Counts by Month ({col})\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Counts by weekday\n",
    "    # ensure Mon–Sun order\n",
    "    weekdays = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "    wd = df_temp['weekday'].value_counts().reindex(weekdays)\n",
    "    plt.figure()\n",
    "    wd.plot(kind='bar')\n",
    "    plt.title(f\"Counts by Weekday ({col})\")\n",
    "    plt.xlabel(\"Weekday\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Distribution of hour of day\n",
    "    plt.figure()\n",
    "    sns.histplot(df_temp['hour'], bins=24, discrete=True, kde=False)\n",
    "    plt.title(f\"Survey Time of Day Distribution ({col})\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(range(0,24))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4062bea4",
   "metadata": {},
   "source": [
    "Analyze new datetime columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48af941-c391-4bc3-b0ac-ec18207c3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS: \n",
    "    print(\"NEPAL DATA\")\n",
    "    analyze_datetime(nepal_df, 'Q2__Surveyed_Date_Time__time')\n",
    "    print(\"SENEGAL DATA\")\n",
    "    analyze_datetime(senegal_df, 'Q2__Surveyed_Date_Time__time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38beb10c-62e6-4f5d-af05-8e8a00c0858c",
   "metadata": {},
   "source": [
    "## Analyze Survey Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c795a0",
   "metadata": {},
   "source": [
    "Add Survey_Length column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f0c14-e60e-4877-9f6f-ae42c288fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# turn time → \"HH:MM:SS\" strings → Timedelta\n",
    "nepal_df['Q0__SurveyLengthTime__time'] = (\n",
    "    nepal_df['Q0__SurveyLengthTime__time']\n",
    "      .astype(str)                # datetime.time → \"HH:MM:SS\"\n",
    "      .pipe(pd.to_timedelta)      # parse into timedelta64[ns]\n",
    ")\n",
    "\n",
    "# first, make sure both your start‐ and end‐times are strings\n",
    "senegal_df['Q2__Surveyed_Date_Time__time'] = senegal_df['Q2__Surveyed_Date_Time__time'].astype(str)\n",
    "senegal_df['Q0__Surveyed_End_Date__time']   = senegal_df['Q0__Surveyed_End_Date__time'].astype(str)\n",
    "\n",
    "# 1) Parse your “start” column as a real timestamp\n",
    "senegal_df['start_dt'] = pd.to_datetime(\n",
    "    senegal_df['Q2__Surveyed_Date_Time__time'],\n",
    "    format='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# 2) Turn your “end time” (just HH:MM:SS) into a Timedelta\n",
    "senegal_df['end_td'] = pd.to_timedelta(\n",
    "    senegal_df['Q0__Surveyed_End_Date__time'].astype(str)\n",
    ")\n",
    "\n",
    "# 3) Build a full “end” timestamp by taking the date from start_dt and adding the time delta\n",
    "senegal_df['end_dt'] = (\n",
    "    senegal_df['start_dt'].dt.normalize()  # midnight of the start date\n",
    "  + senegal_df['end_td']\n",
    ")\n",
    "\n",
    "senegal_df['Q0__SurveyLengthTime__time'] = senegal_df['end_dt'] - senegal_df['start_dt']\n",
    "\n",
    "senegal_df.drop(columns=['start_dt', 'end_td', 'end_dt'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f005848",
   "metadata": {},
   "source": [
    "Function to analyze Survey_Length column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdded1a-ce51-4ca7-ae13-b8c828a996ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def analyze_survey_length(df, column='Q0__SurveyLengthTime__time', bins=30):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of survey lengths in a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the survey length column.\n",
    "    column : str, default 'Survey_Length'\n",
    "        Name of the timedelta column to analyze.\n",
    "    bins : int, default 30\n",
    "        Number of histogram bins.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    - Histogram of survey lengths (HH:MM:SS) distribution.\n",
    "    - Printed mean and standard deviation of survey lengths.\n",
    "    - Returns a tuple (mean_timedelta, std_timedelta).\n",
    "    \"\"\"\n",
    "    # Ensure the column is timedelta64[ns]\n",
    "    if not pd.api.types.is_timedelta64_dtype(df[column]):\n",
    "        raise TypeError(f\"Column '{column}' must be timedelta64[ns] dtype\")\n",
    "\n",
    "    # Compute statistics\n",
    "    mean_td = df[column].mean()\n",
    "    std_td = df[column].std()\n",
    "\n",
    "    # Convert to total seconds for plotting\n",
    "    secs = df[column].dt.total_seconds()\n",
    "\n",
    "    # Plot histogram\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(secs, bins=bins)\n",
    "    ax.set_title(\"Distribution of Survey Lengths\")\n",
    "    ax.set_xlabel(\"Survey Length (HH:MM:SS)\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Formatter to convert seconds back to HH:MM:SS\n",
    "    def sec_to_hhmmss(x, pos):\n",
    "        h = int(x // 3600)\n",
    "        m = int((x % 3600) // 60)\n",
    "        s = int(x % 60)\n",
    "        return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(sec_to_hhmmss))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print mean and std in HH:MM:SS\n",
    "    mean_str = str(mean_td).split()[-1]\n",
    "    std_str  = str(std_td).split()[-1]\n",
    "    print(f\"Mean survey length: {mean_str}\")\n",
    "    print(f\"Std dev survey length: {std_str}\")\n",
    "\n",
    "    return mean_td, std_td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c35ec-21a2-44fe-be73-b8376dd2daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS:\n",
    "    print(\"NEPAL DATA\")\n",
    "    analyze_survey_length(nepal_df)\n",
    "    print(\"SENEGAL DATA\")\n",
    "    analyze_survey_length(senegal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bc008",
   "metadata": {},
   "source": [
    "Discovered data-entry glitch extreme value at one Senegalese survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570bc6e6-82fd-4f4f-8a5b-74eb374283b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = senegal_df[ senegal_df['Q0__SurveyLengthTime__time'] < pd.Timedelta(0) ]\n",
    "print(bad[['Q2__Surveyed_Date_Time__time','Q0__Surveyed_End_Date__time','Q0__SurveyLengthTime__time']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a2f55",
   "metadata": {},
   "source": [
    "Filling in this survey with median survey length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4295a-8a1c-4d1c-944a-f134db186624",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_td = senegal_df.loc[ senegal_df['Q0__SurveyLengthTime__time'] >= pd.Timedelta(0), 'Q0__SurveyLengthTime__time' ].median()\n",
    "mask = senegal_df['Q0__SurveyLengthTime__time'] < pd.Timedelta(0)\n",
    "print(f\"Found {mask.sum()} negative Survey_Length(s), filling with median = {median_td}\")\n",
    "senegal_df.loc[mask, 'Q0__SurveyLengthTime__time'] = median_td"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7331de",
   "metadata": {},
   "source": [
    "Redefine survey analysis function by adding standard deviation markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c956e20-3ac3-4efe-ba2c-f74283e6da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def analyze_survey_length(df, column='Q0__SurveyLengthTime__time', bins=30, max_std=3):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of survey lengths in a DataFrame,\n",
    "    and overlay vertical lines at mean ± n * std for n = 1..max_std.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the survey length column.\n",
    "    column : str, default 'Survey_Length'\n",
    "        Name of the timedelta column to analyze.\n",
    "    bins : int, default 30\n",
    "        Number of histogram bins.\n",
    "    max_std : int, default 3\n",
    "        How many standard-deviation multiples to draw lines for.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    - Histogram of survey lengths (HH:MM:SS) distribution with σ-lines.\n",
    "    - Printed mean and standard deviation of survey lengths.\n",
    "    - Returns a tuple (mean_timedelta, std_timedelta).\n",
    "    \"\"\"\n",
    "    # 1) Type check\n",
    "    if not pd.api.types.is_timedelta64_dtype(df[column]):\n",
    "        raise TypeError(f\"Column '{column}' must be timedelta64[ns] dtype\")\n",
    "\n",
    "    # 2) Compute statistics\n",
    "    mean_td = df[column].mean()\n",
    "    std_td  = df[column].std()\n",
    "\n",
    "    mean_sec = mean_td.total_seconds()\n",
    "    std_sec  = std_td.total_seconds()\n",
    "\n",
    "    # 3) Build histogram\n",
    "    secs = df[column].dt.total_seconds()\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.hist(secs, bins=bins, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(\"Distribution of Survey Lengths\")\n",
    "    ax.set_xlabel(\"Survey Length (HH:MM:SS)\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Formatter: seconds → HH:MM:SS\n",
    "    def sec_to_hhmmss(x, pos):\n",
    "        h = int(x // 3600)\n",
    "        m = int((x % 3600) // 60)\n",
    "        s = int(x % 60)\n",
    "        return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(sec_to_hhmmss))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # 4) Overlay mean and std-bands\n",
    "    #    use different linestyles/colors for clarity\n",
    "    for n in range(0, max_std+1):\n",
    "        # skip n=0 since that’s the mean line (draw it separately for legend clarity)\n",
    "        if n == 0:\n",
    "            ax.axvline(mean_sec, color='black', linestyle='-', linewidth=2,\n",
    "                       label='Mean')\n",
    "        else:\n",
    "            for sign, label in [(+1, f'+{n}σ'), (-1, f'-{n}σ')]:\n",
    "                x = mean_sec + sign * n * std_sec\n",
    "                ax.axvline(x, linestyle='--' if n==1 else ':',\n",
    "                           linewidth=1.5 if n==1 else 1,\n",
    "                           label=label if sign>0 else None)\n",
    "    # Only one -σ label needed, so we skip labeling the negative side\n",
    "\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 5) Print out\n",
    "    mean_str = str(mean_td).split()[-1]\n",
    "    std_str  = str(std_td).split()[-1]\n",
    "    print(f\"Mean survey length: {mean_str}\")\n",
    "    print(f\"Std dev survey length: {std_str}\")\n",
    "\n",
    "    return mean_td, std_td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35407d79-a680-4d20-87d5-9eac98c0b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS:\n",
    "    print(\"SENEGAL DATA\")\n",
    "    analyze_survey_length(senegal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e6b04",
   "metadata": {},
   "source": [
    "Outliers defined as beyond 3 standard deviations, we swap outliers with median survey length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b58f0-db8e-40ac-8c5d-c05f42edb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = senegal_df['Q0__SurveyLengthTime__time'].mean() + 2*senegal_df['Q0__SurveyLengthTime__time'].std()\n",
    "mask = senegal_df['Q0__SurveyLengthTime__time'] > threshold\n",
    "print(f\"Changed {mask.sum()} outlier survey lengths to median\")\n",
    "senegal_df.loc[mask, 'Q0__SurveyLengthTime__time'] = median_td"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6afd0",
   "metadata": {},
   "source": [
    "Recheck survey length histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b4039-77d0-44df-9b08-c307f4f74e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS:\n",
    "    print(\"SENEGAL DATA\")\n",
    "    analyze_survey_length(senegal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ad0ff-3968-4fe5-9020-3af1540311f1",
   "metadata": {},
   "source": [
    "## Analyze Locational Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef9623c",
   "metadata": {},
   "source": [
    "Since added Survey_Length column, took out others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8402b2-d4d7-4488-9b56-ba0a0137640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nepal_df = nepal_df.drop(['Q0__SurveyedEndTime__time'], axis=1)\n",
    "senegal_df = senegal_df.drop(['Q0__Surveyed_End_Date__time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e6469",
   "metadata": {},
   "source": [
    "Showing missing columns in the Senegal dataset's Location_... columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30667ccb-75c1-435a-8d51-d3e38f7e0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Latitude', 'Longitude', 'Altitude', 'Accuracy']:\n",
    "    print(f\"missing values in Q1__Location_{column}: {senegal_df[f'Q1__Location_{column}__continuous'].isna().sum()}\")\n",
    "    print(f\"missing values in {column}: {senegal_df[f'Q1__{column}__continuous'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9dcdf",
   "metadata": {},
   "source": [
    "Removing Location_ columns from senegal data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad853b-d0d9-410b-ab10-cd481d4f2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "senegal_df = senegal_df.drop(['Q1__Location_Latitude__continuous', 'Q1__Location_Longitude__continuous', 'Q1__Location_Altitude__continuous', 'Q1__Location_Accuracy__continuous'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca43ce4",
   "metadata": {},
   "source": [
    "Both are ok for nepal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed6ab4-eb14-44bc-a6c9-dbeb62075545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Latitude', 'Longitude', 'Altitude', 'Accuracy']:\n",
    "    print(f\"missing values in Location_{column}: {nepal_df[f'Q1__Location_{column}__continuous'].isna().sum()}\")\n",
    "    print(f\"missing values in {column}: {nepal_df[f'Q1__{column}__continuous'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e676bbfb",
   "metadata": {},
   "source": [
    "Still removing from nepal Location_ columns for consistency between both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9249852-d6d5-4a72-bb03-8dbc1e438488",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_df = nepal_df.drop(['Q1__Location_Latitude__continuous', 'Q1__Location_Longitude__continuous', 'Q1__Location_Altitude__continuous', 'Q1__Location_Accuracy__continuous'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963536e",
   "metadata": {},
   "source": [
    "Defining function to analyze locational columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749cede-e5f2-4f59-b52c-e4b8d010a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "def lat_lon_hillshade_map(df, zoom_start=11):\n",
    "    \"\"\"\n",
    "    Folium map with:\n",
    "      • Default OpenStreetMap layer\n",
    "      • ESRI World Hillshade (true terrain) layer\n",
    "      • MarkerCluster of your points with Altitude pop-ups\n",
    "      • LayerControl for toggling back and forth\n",
    "    \"\"\"\n",
    "    # 1) Prepare & center\n",
    "    df_map = df.dropna(subset=['Q1__Latitude__continuous','Q1__Longitude__continuous']).copy()\n",
    "    center = [df_map['Q1__Latitude__continuous'].mean(), df_map['Q1__Longitude__continuous'].mean()]\n",
    "\n",
    "    # 2) Build base map (OSM by default)\n",
    "    m = folium.Map(location=center, zoom_start=zoom_start)\n",
    "\n",
    "    # 3) Add ESRI World Hillshade as a second base layer\n",
    "    folium.TileLayer(\n",
    "        tiles='https://server.arcgisonline.com/ArcGIS/rest/services/'\n",
    "              'Elevation/World_Hillshade/MapServer/tile/{z}/{y}/{x}',\n",
    "        attr='Tiles © Esri — Source: USGS',\n",
    "        name='Hillshade (ESRI)'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # 4) Your markers (clustered)\n",
    "    mc = MarkerCluster(name='Points').add_to(m)\n",
    "    for idx, row in df_map.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row['Q1__Latitude__continuous'], row['Q1__Longitude__continuous']],\n",
    "            popup=(\n",
    "                f\"<b>Index:</b> {idx}<br>\"\n",
    "                f\"<b>Altitude:</b> {row.get('Q1__Altitude__continuous','N/A')} m\"\n",
    "            )\n",
    "        ).add_to(mc)\n",
    "\n",
    "    # 5) Toggle control\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b20dc",
   "metadata": {},
   "source": [
    "Analyze lat long data for nepal, both openstreetmap and hillshade for elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc78c5-1516-44ee-aaa6-72640660a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS:\n",
    "    nepal_map = lat_lon_hillshade_map(nepal_df)\n",
    "    nepal_map   # in Jupyter this will render with two base‐layers to switch between\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10506a",
   "metadata": {},
   "source": [
    "The same for Senegal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00523075-5b8e-4e65-8a44-754fd75e3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS:\n",
    "    print(\"SENEGAL DATA\")\n",
    "    senegal_map = lat_lon_hillshade_map(senegal_df)\n",
    "    senegal_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04dbfe",
   "metadata": {},
   "source": [
    "Check against google if the altitude column is accurate given the lat lot coordinates, plot accuracy also to see if there is some connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb5d68-2bbe-4b1b-b1b6-6c1c89070a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def plot_altitude_error(df, lat_col='Q1__Latitude__continuous', lon_col='Q1__Longitude__continuous', alt_col='Q1__Altitude__continuous', accuracy_col='Q1__Accuracy__continuous', api_url='https://api.open-elevation.com/api/v1/lookup', batch_size=100, bins=30):\n",
    "    '''\n",
    "    Fetches true elevations and plots a combined view showing:\n",
    "      - Histogram of altitude errors (recorded - true).\n",
    "      - Median reported accuracy for each error bin overlaid as a line.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with latitude, longitude, recorded altitude, and accuracy columns.\n",
    "    lat_col (str): Name of the latitude column.\n",
    "    lon_col (str): Name of the longitude column.\n",
    "    alt_col (str): Name of the recorded altitude column.\n",
    "    accuracy_col (str): Name of the accuracy column.\n",
    "    api_url (str): Elevation API endpoint.\n",
    "    batch_size (int): Number of locations per API request.\n",
    "    bins (int): Number of bins for error histogram and averaging.\n",
    "\n",
    "    Returns:\n",
    "    None. Displays a joint plot of error distribution and median accuracy per error bin.\n",
    "    '''\n",
    "    # Work on a copy\n",
    "    df = df.copy()\n",
    "    true_alts = []\n",
    "\n",
    "    # Batch request loop to fetch true elevations\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        locations = '|'.join(f\"{lat},{lon}\" for lat, lon in zip(batch[lat_col], batch[lon_col]))\n",
    "        resp = requests.get(api_url, params={'locations': locations})\n",
    "        resp.raise_for_status()\n",
    "        results = resp.json().get('results', [])\n",
    "        if len(results) != len(batch):\n",
    "            raise ValueError(f\"Expected {len(batch)} results, got {len(results)}\")\n",
    "        true_alts.extend(r['elevation'] for r in results)\n",
    "\n",
    "    # Compute errors and extract accuracy\n",
    "    recorded = df[alt_col].values\n",
    "    errors = recorded - np.array(true_alts)\n",
    "    accuracies = df[accuracy_col].values\n",
    "\n",
    "    # Prepare bins and compute median accuracy per bin\n",
    "    bin_edges = np.linspace(errors.min(), errors.max(), bins + 1)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    digitized = np.digitize(errors, bin_edges) - 1\n",
    "    median_acc = [np.median(accuracies[digitized == i]) if np.any(digitized == i) else np.nan for i in range(bins)]\n",
    "\n",
    "    # Create combined plot\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Histogram of errors\n",
    "    ax1.hist(errors, bins=bin_edges, alpha=0.6)\n",
    "    ax1.set_xlabel('Altitude Error (m)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.axvline(0, color='k', linestyle='--')\n",
    "\n",
    "\n",
    "    plt.title('Altitude Error Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e589b22-d596-4723-9eb0-3ba37e153bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS:\n",
    "    plot_altitude_error(nepal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92c5e5-cc9e-43fd-a2a7-4d9a17a8ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS:\n",
    "    plot_altitude_error(senegal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc438c0",
   "metadata": {},
   "source": [
    "Looks like gaussian noise error, adding approximate average to center errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a14ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_df[\"Q1__Altitude__continuous\"] = nepal_df[\"Q1__Altitude__continuous\"] + 62\n",
    "senegal_df[\"Q1__Altitude__continuous\"] = senegal_df[\"Q1__Altitude__continuous\"] - 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS:\n",
    "    plot_altitude_error(nepal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_GRAPHS:\n",
    "    plot_altitude_error(senegal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdfc431",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365fea2",
   "metadata": {},
   "source": [
    "View missing counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf79edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts_nepal = nepal_df.isnull().sum()\n",
    "print(\"Missing values per column in nepal:\")\n",
    "print(missing_counts_nepal[missing_counts_nepal>0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts_senegal = senegal_df.isnull().sum()\n",
    "print(\"Missing values per column in senegal:\")\n",
    "print(missing_counts_senegal[missing_counts_senegal>0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00544b2",
   "metadata": {},
   "source": [
    "Drop almost completely missing columns in Senegal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c45661",
   "metadata": {},
   "outputs": [],
   "source": [
    "senegal_df = senegal_df.drop(columns=[\"Q0__Survey_duration__time\", \"Q89__education_level__nominal\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88725a78",
   "metadata": {},
   "source": [
    "We remove these columns because our coordinator said we should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "children_cols = ['Q102__How_many_children_do_you_have__continuous', 'Q103__How_many_children_do_you_plan_to_have_in_your_whole_live__continuous']\n",
    "\n",
    "nepal_df = nepal_df.drop(columns=children_cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c5236",
   "metadata": {},
   "source": [
    "Seperate numeric and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_missing_cols = nepal_df.columns[nepal_df.isnull().any() == True]\n",
    "\n",
    "\n",
    "nepal_num_cols = [c for c in nepal_df.columns if parse_feature_metadata(c)[\"type\"] == \"continuous\"]\n",
    "nepal_cat_cols = [c for c in nepal_df.columns if parse_feature_metadata(c)[\"type\"] not in [\"continuous\", \"time\"]]\n",
    "nepal_num_missing_cols = [c for c in nepal_missing_cols if c not in nepal_cat_cols]\n",
    "\n",
    "\n",
    "senegal_missing_cols = senegal_df.columns[senegal_df.isnull().any() == True]\n",
    "\n",
    "senegal_num_cols = [c for c in senegal_df.columns if parse_feature_metadata(c)[\"type\"] == \"continuous\"]\n",
    "senegal_cat_cols = [c for c in senegal_df.columns if parse_feature_metadata(c)[\"type\"] not in [\"continuous\", \"time\"]]\n",
    "senegal_num_missing_cols = [c for c in senegal_missing_cols if c not in senegal_cat_cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38213509",
   "metadata": {},
   "source": [
    "We use MICE imputation to deal with missing numeric values in both datasets, and impute missing categorical values with the mode of that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "mice_imp = mice_imp = IterativeImputer(\n",
    "    random_state=123,\n",
    "    sample_posterior=True,    # draws from the posterior → multiple imputation flavour\n",
    "    max_iter=10,\n",
    "    n_nearest_features=5\n",
    ")\n",
    "\n",
    "imp_n_df = nepal_df.copy()\n",
    "imp_s_df = senegal_df.copy()\n",
    "\n",
    "imp_n_df[nepal_num_cols] = mice_imp.fit_transform(imp_n_df[nepal_num_cols])\n",
    "imp_s_df[senegal_num_cols] = mice_imp.fit_transform(imp_s_df[senegal_num_cols])\n",
    "\n",
    "mode_imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# turn every Python None into a real NaN in your categorical block\n",
    "imp_n_df[nepal_cat_cols] = imp_n_df[nepal_cat_cols] \\\n",
    "    .replace({None: np.nan})\n",
    "\n",
    "imp_n_df.loc[:, nepal_cat_cols] = mode_imp.fit_transform(imp_n_df[nepal_cat_cols])\n",
    "imp_s_df.loc[:, senegal_cat_cols] = mode_imp.fit_transform(imp_s_df[senegal_cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443256e",
   "metadata": {},
   "source": [
    "Check that numeric column imputations are plausible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in nepal_num_missing_cols:\n",
    "    sns.kdeplot(nepal_df[col].dropna(), label=\"obs\")\n",
    "    sns.kdeplot(imp_n_df[col], label=\"imp\")\n",
    "    plt.legend(); plt.title(col); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in senegal_num_missing_cols:\n",
    "    sns.kdeplot(senegal_df[col].dropna(), label=\"obs\")\n",
    "    sns.kdeplot(imp_s_df[col], label=\"imp\")\n",
    "    plt.legend(); plt.title(col); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ae7df",
   "metadata": {},
   "source": [
    "Apply to original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_df = imp_n_df\n",
    "senegal_df = imp_s_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736cd5fc",
   "metadata": {},
   "source": [
    "Recheck missing values in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3397b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts_nepal = nepal_df.isnull().sum()\n",
    "print(\"Missing values per column in nepal:\")\n",
    "print(missing_counts_nepal[missing_counts_nepal>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf875302",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts_senegal = senegal_df.isnull().sum()\n",
    "print(\"Missing values per column in senegal:\")\n",
    "print(missing_counts_senegal[missing_counts_senegal>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f190d87",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873dad0",
   "metadata": {},
   "source": [
    "First visualize the distributions of numeric columns with boxplots and histograms and also view their skew metrics and how many IQR-based outliers are detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a35e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew        # skewness metric\n",
    "\n",
    "\n",
    "def visualize_num_columns(df, num_cols, country):\n",
    "    for col in num_cols:\n",
    "        series = df[col].dropna()\n",
    "\n",
    "        # ── 1. Visualise ──────────────────────────────────────────────────────────\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        sns.boxplot(y=series, ax=ax[0])\n",
    "        ax[0].set_title(f\"{col} • Boxplot\")\n",
    "\n",
    "        sns.histplot(series, kde=True, ax=ax[1])\n",
    "        ax[1].set_title(f\"{col} • Histogram / KDE\")\n",
    "\n",
    "        plt.suptitle(f\"{country}: {col}\", fontsize=11, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # ── 2. Skewness ──────────────────────────────────────────────────────────\n",
    "        skew_val = skew(series)\n",
    "        print(f\"  • Skewness of '{col}': {skew_val:+.3f}\")\n",
    "\n",
    "        # ── 3. IQR-based outlier detection ──────────────────────────────────────\n",
    "        Q1, Q3 = series.quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        mask_outliers = (series < lower) | (series > upper)\n",
    "        n_outliers = mask_outliers.sum()\n",
    "\n",
    "        print(f\"  • IQR outliers in '{col}': {n_outliers} \"\n",
    "              f\"(lower < {lower:.3f}, upper > {upper:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Pair each DataFrame with its list of numeric columns and a label\n",
    "dfs_and_cols = [\n",
    "    (nepal_df,   nepal_num_cols,   \"Nepal\"),\n",
    "    (senegal_df, senegal_num_cols, \"Senegal\")\n",
    "]\n",
    "\n",
    "for df, num_cols, country in dfs_and_cols:\n",
    "    print(f\"\\nEvaluating outliers in numerical columns of the {country} dataset\")\n",
    "\n",
    "    visualize_num_columns(df, num_cols, country)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31441942",
   "metadata": {},
   "source": [
    "Avoided as much as possible winsorization, applied log transformations when skew was over 1.0, between 0.5 and 1.0 applied sqrt transformation, below 0.5 left as is.\n",
    "Will use bicor or spearman correlation in EFA to mitigate effect of remaining outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c879779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "df = nepal_df.copy()\n",
    "\n",
    "def new_name(col, tag):\n",
    "    \"\"\"Insert _log or _sqrt right before '__continuous'.\"\"\"\n",
    "    return col.replace(\"__continuous\", f\"{tag}__continuous\")\n",
    "\n",
    "# signed-log1p for variables that can be negative\n",
    "signed_log1p = lambda x: np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "# 1) Altitude  ────────────────────────────────────────────────────────────\n",
    "col = \"Q1__Altitude__continuous\"\n",
    "upper = df[col].quantile(0.99)\n",
    "df[new_name(col, \"_log\")] = np.log1p(df[col].clip(lower=0, upper=upper))\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 2) Rented land (bigha)  ────────────────────────────────────────────────\n",
    "col = \"Q51__How_much_land_that_is_rented_or_leased_do_you_cultivate_bigha__continuous\"\n",
    "df[new_name(col, \"_log\")] = np.log1p(df[col])\n",
    "\n",
    "ord_name = col.replace(\"__continuous\", \"__ordinal\")\n",
    "df[ord_name] = pd.cut(\n",
    "    df[col],\n",
    "    bins=[-np.inf, 0, 1.5, np.inf],\n",
    "    labels=[\"none\", \"little\", \"much\"],\n",
    "    include_lowest=True,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Drop the original continuous column\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "\n",
    "# 3) Vegetable plot size (bigha)  ────────────────────────────────────────\n",
    "col = \"Q52__On_how_much_land_do_you_grow_vegetables_bigha__continuous\"\n",
    "df[new_name(col, \"_log\")] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 4) Vegetable harvest (kg)  ─────────────────────────────────────────────\n",
    "col = \"Q62__How_much_VEGETABLES_do_you_harvest_per_year_from_this_plot_kilograms__continuous\"\n",
    "df[new_name(col, \"_log\")] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 5) Drip-irrigation years → binary  ─────────────────────────────────────\n",
    "col = \"Q78__How_many_years_have_you_been_using_drip_irrigation_if_no_zero__continuous\"\n",
    "df[col.replace(\"__continuous\", \"__binary__1\")] = (df[col] > 0).astype(int)\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 6) Household size  ─────────────────────────────────────────────────────\n",
    "col = \"Q101__how_many_people_live_in_this_household__continuous\"\n",
    "df[new_name(col, \"_log\")] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 7) Wife/husband education years  ───────────────────────────────────────\n",
    "col = \"Q107__Education_level_of_your_wife_husband_How_many_years_of_formal_education__continuous\"\n",
    "df[new_name(col, \"_log\")] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 8) Agriculture income (remove 1 extreme, then log)  ────────────────────\n",
    "col = \"Q108__What_is_your_households_yearly_income_from_agriculture_NPR__continuous\"\n",
    "# new: remove the row with the extreme value entirely\n",
    "idx = df[col].idxmax()\n",
    "df.drop(index=idx, inplace=True)\n",
    "# log transform\n",
    "df[new_name(col, \"_log\")] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 9) Overall income  ─────────────────────────────────────────────────────\n",
    "col = \"Q109__What_is_your_households_yearly_income_overall_including_agriculture_NPR__continuous\"\n",
    "df[new_name(col, \"_log\")] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 10) Loan amount  ───────────────────────────────────────────────────────\n",
    "col = \"Q110__Do_you_currently_have_a_loan_if_no_zero_if_yes_amount_of_money_loaned_NPR__continuous\"\n",
    "ord_name = col.replace(\"__continuous\", \"__ordinal\")\n",
    "df[ord_name] = pd.cut(\n",
    "    df[col],\n",
    "    bins=[-np.inf, 0, 1, np.inf],\n",
    "    labels=[\"none\", \"little\", \"much\"],\n",
    "    include_lowest=True,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Drop the original continuous column\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 11) Remove coordinator-flagged totals  ─────────────────────────────────\n",
    "df.drop(columns=[\n",
    "    \"Q0__positive_total__continuous\",\n",
    "    \"Q0__negative_total__continuous\"\n",
    "], errors=\"ignore\", inplace=True)\n",
    "\n",
    "# 12) Distance  ──────────────────────────────────────────────────────────\n",
    "col = \"Q0__Distance__continuous\"\n",
    "df[new_name(col, \"_log\")] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 13) Total area (sqrt)  ─────────────────────────────────────────────────\n",
    "col = \"Q0__TOTAL_AREA__continuous\"\n",
    "df[new_name(col, \"_sqrt\")] = np.sqrt(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# df now holds the transformed / recoded columns with correct naming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = []\n",
    "for col in df:\n",
    "    if \"__continuous\" in col:\n",
    "        num_cols.append(col)\n",
    "    \n",
    "visualize_num_columns(df, num_cols, \"Nepal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a37e9",
   "metadata": {},
   "source": [
    "Used light winsorization in specific cases, applied log transformations when skew was over 1.0, between 0.5 and 1.0 applied sqrt transformation, below 0.5 left as is. Will use bicor or spearman correlation in EFA to mitigate effect of remaining outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45042c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# work on a copy\n",
    "df = senegal_df.copy()\n",
    "\n",
    "# ── helper functions ─────────────────────────────────────────────────────────\n",
    "def rename_cont(col, tag):\n",
    "    \"\"\"\n",
    "    Insert a tag (_log, _sqrt) just before the '_continuous' suffix.\n",
    "    \"\"\"\n",
    "    if col.endswith(\"__continuous\"):\n",
    "        return col.replace(\"__continuous\", f\"{tag}__continuous\")\n",
    "    elif col.endswith(\"_continuous\"):\n",
    "        return col.replace(\"_continuous\", f\"{tag}_continuous\")\n",
    "    else:\n",
    "        raise ValueError(f\"Column {col} does not end with '_continuous'\")\n",
    "\n",
    "def tukey_fence(series):\n",
    "    \"\"\"\n",
    "    Compute lower and upper Tukey fences for a pandas Series.\n",
    "    Returns (lower, upper).\n",
    "    \"\"\"\n",
    "    q1, q3 = series.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    return q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "\n",
    "# ── 1. Q1_Altitude_continuous: winsorize at Tukey fences ────────────────────\n",
    "col = \"Q1__Altitude__continuous\"\n",
    "low, high = tukey_fence(df[col].dropna())\n",
    "df[col] = df[col].clip(lower=low, upper=high)\n",
    "\n",
    "# ── 2. Q0__Hope_total__continuous: clip upper tail at 54 ────────────────────\n",
    "col = \"Q0__Hope_total__continuous\"\n",
    "df[col] = df[col].clip(upper=54)\n",
    "\n",
    "# ── 3. Q0__Average_CS__continuous: clip upper tail at 4.5 ──────────────────\n",
    "col = \"Q0__Average_CS__continuous\"\n",
    "df[col] = df[col].clip(upper=4.5)\n",
    "\n",
    "# ── 4. Q61__Land_rented_cultivated_ha__continuous: log1p transform ─────────\n",
    "col = \"Q61__Land_rented_cultivated_ha__continuous\"\n",
    "new = rename_cont(col, \"_log\")\n",
    "df[new] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# ── 5. Q71__VEG_harvest_per_year_kg__continuous: log1p transform ───────────\n",
    "col = \"Q71__VEG_harvest_per_year_kg__continuous\"\n",
    "new = rename_cont(col, \"_log\")\n",
    "df[new] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# ── 6. Q84__Years_using_drip_irrigation__continuous: log1p transform ───────\n",
    "col = \"Q84__Years_using_drip_irrigation__continuous\"\n",
    "new = rename_cont(col, \"_log\")\n",
    "df[new] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# ── 7. Q87__Household_size__continuous: sqrt transform ─────────────────────\n",
    "col = \"Q87__Household_size__continuous\"\n",
    "new = rename_cont(col, \"_sqrt\")\n",
    "df[new] = np.sqrt(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# ── 8. Q0__Education_years__continuous: log1p transform ────────────────────\n",
    "col = \"Q0__Education_years__continuous\"\n",
    "new = rename_cont(col, \"_log\")\n",
    "df[new] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# ── 9. Q90__Yearly_income_agriculture_XOF__continuous:\n",
    "#     drop the two largest outliers, then log1p transform ───────────────────\n",
    "col = \"Q90__Yearly_income_agriculture_XOF__continuous\"\n",
    "# identify and drop the two rows with the highest values\n",
    "outliers = df[col].nlargest(2).index\n",
    "df.drop(index=outliers, inplace=True)\n",
    "# now transform\n",
    "new = rename_cont(col, \"_log\")\n",
    "df[new] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# ── 10. Q91__Yearly_income_overall_XOF__continuous: log1p transform ─────────\n",
    "col = \"Q91__Yearly_income_overall_XOF__continuous\"\n",
    "new = rename_cont(col, \"_log\")\n",
    "df[new] = np.log1p(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# ── Q92: Current loan amount ────────────────────────────────────────────────\n",
    "col = \"Q92__Current_loan_amount_XOF__continuous\"\n",
    "\n",
    "# Create a 3‐level ordinal band: \n",
    "#    - 'near'   = no loan (0 XOF)\n",
    "#    - 'medium' = loan > 0 up to the median of positive loans\n",
    "#    - 'far'    = loan above that median\n",
    "ord_name = col.replace(\"__continuous\", \"__ordinal\")\n",
    "df[ord_name] = pd.cut(\n",
    "    df[col],\n",
    "    bins=[-np.inf, 0, 1e6, np.inf],\n",
    "    labels=[\"near\", \"medium\", \"far\"],\n",
    "    include_lowest=True,\n",
    ")\n",
    "\n",
    "# 3) Drop the original continuous column\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# ── 12. Q0__Distance_Dakar_KM__continuous: sqrt transform ──────────────────\n",
    "col = \"Q0__Distance_Dakar_KM__continuous\"\n",
    "new = rename_cont(col, \"_sqrt\")\n",
    "df[new] = np.sqrt(df[col])\n",
    "df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# ── 13. Q0__Age_Education_interaction__continuous: winsorize ───────────────\n",
    "col = \"Q0__Age_Education_interaction__continuous\"\n",
    "low, high = tukey_fence(df[col].dropna())\n",
    "df[col] = df[col].clip(lower=low, upper=high)\n",
    "\n",
    "# ── 'df' now contains your Senegal data with all requested transforms ───────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ce604",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = []\n",
    "for col in df:\n",
    "    if \"__continuous\" in col:\n",
    "        num_cols.append(col)\n",
    "    \n",
    "visualize_num_columns(df, num_cols, \"Senegal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "senegal_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e9485",
   "metadata": {},
   "source": [
    "# Create Productivity Metric Columns (Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    return (x - x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cae60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NPL_YEARLY_AGR_INCOME_COL = \"Q108__What_is_your_households_yearly_income_from_agriculture_NPR_log__continuous\"\n",
    "SNGL_YEARLY_AGR_INCOME_COL = 'Q90__Yearly_income_agriculture_XOF_log__continuous'\n",
    "NPL_OWND_CULTVTD_LAND_COL = \"Q50__How_much_land_that_is_yours_do_you_cultivate_bigha__continuous\"\n",
    "SNGL_OWND_CULTVTD_LAND_COL = \"Q60__Land_owned_cultivated_ha__continuous\"\n",
    "NPL_LEAS_CULTVTD_LAND_COL = \"Q51__How_much_land_that_is_rented_or_leased_do_you_cultivate_bigha_log__continuous\"\n",
    "SNGL_LEAS_CULTVTD_LAND_COL = \"Q61__Land_rented_cultivated_ha_log__continuous\"\n",
    "NPL_VEG_HARVEST_PER_YEAR_COL = \"Q62__How_much_VEGETABLES_do_you_harvest_per_year_from_this_plot_kilograms_log__continuous\"\n",
    "SNGL_VEG_HARVEST_PER_YEAR_COL = \"Q71__VEG_harvest_per_year_kg_log__continuous\"\n",
    "NPL_SELF_REPORTED_FARM_LEVEL_COL = \"Q112__Generally_speaking_how_would_you_define_your_farming__ordinal\"\n",
    "SNGL_SELF_REPORTED_FARM_LEVEL_COL = \"Q94__Farming_level_relative__ordinal\"\n",
    "NPL_VEG_LAND = \"Q52__On_how_much_land_do_you_grow_vegetables_bigha_log__continuous\"\n",
    "SNGL_VEG_LAND = \"Q62__Land_grow_vegetables_ha__continuous\"\n",
    "\n",
    "nepal_df['yearly_income_from_agr_per_land_Z'] = ((nepal_df[NPL_YEARLY_AGR_INCOME_COL].transform(standardize)) / (nepal_df[NPL_OWND_CULTVTD_LAND_COL].transform(standardize) + nepal_df[NPL_LEAS_CULTVTD_LAND_COL].transform(standardize))).transform(standardize)\n",
    "senegal_df['yearly_income_from_agr_per_land_Z'] = ((senegal_df[SNGL_YEARLY_AGR_INCOME_COL].transform(standardize)) / (senegal_df[SNGL_OWND_CULTVTD_LAND_COL].transform(standardize) + senegal_df[SNGL_LEAS_CULTVTD_LAND_COL].transform(standardize))).transform(standardize)\n",
    "\n",
    "nepal_df['yearly_income_from_agr_Z'] = nepal_df[NPL_YEARLY_AGR_INCOME_COL].transform(standardize)\n",
    "senegal_df['yearly_income_from_agr_Z'] = senegal_df[SNGL_YEARLY_AGR_INCOME_COL].transform(standardize)\n",
    "\n",
    "nepal_df['veg_per_area_Z'] = (nepal_df[NPL_VEG_HARVEST_PER_YEAR_COL].transform(standardize) / nepal_df[NPL_VEG_LAND].transform(standardize)).transform(standardize)\n",
    "senegal_df['veg_per_area_Z'] = (senegal_df[SNGL_VEG_HARVEST_PER_YEAR_COL].transform(standardize) / senegal_df[SNGL_VEG_LAND].transform(standardize)).transform(standardize)\n",
    "\n",
    "nepal_df['self_farming_perception_Z'] = nepal_df[NPL_SELF_REPORTED_FARM_LEVEL_COL].transform(standardize)\n",
    "senegal_df['self_farming_perception_Z'] = senegal_df[SNGL_SELF_REPORTED_FARM_LEVEL_COL].transform(standardize)\n",
    "\n",
    "# defragment\n",
    "nepal_df = nepal_df.copy()\n",
    "senegal_df = senegal_df.copy()\n",
    "\n",
    "target_cols = ['yearly_income_from_agr_per_land_Z',\n",
    "               'yearly_income_from_agr_Z',\n",
    "               'veg_per_area_Z',\n",
    "               'self_farming_perception_Z']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8331646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from typing import Literal\n",
    "\n",
    "def reduce_to_index(\n",
    "        df: pd.DataFrame,\n",
    "        cols: list[str],\n",
    "        index_name: str,\n",
    "        n_components: int = 1,\n",
    "        collapse_method: Literal[\"keep\", \"weighted-avg\"] = \"keep\"\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Runs PCA on `cols`, retains the first `n_components`, and either\n",
    "    keeps them as separate columns or collapses them into one index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : original DataFrame (unchanged)\n",
    "    cols : list of column names to run PCA on\n",
    "    index_name : base name for new column(s)\n",
    "    n_components : how many PCs to keep (1 or 2 for your case)\n",
    "    collapse_method :\n",
    "        \"keep\"         -> add PC1, PC2, ... separately\n",
    "        \"weighted-avg\" -> single index = Σ w_i * PC_i, w_i = var-ratio_i\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_out : DataFrame with original cols dropped & new columns added\n",
    "    loadings_df : each variable’s loading on the retained PCs\n",
    "    eigenvals : eigenvalues of retained PCs\n",
    "    var_ratios : proportion of variance explained by each retained PC\n",
    "    \"\"\"\n",
    "    if n_components < 1:\n",
    "        raise ValueError(\"n_components must be >= 1\")\n",
    "\n",
    "    # 1. Fit PCA\n",
    "    pca = PCA(n_components=n_components).fit(df[cols])\n",
    "    scores      = pca.transform(df[cols])            # shape (n_samples, n_components)\n",
    "    eigenvals   = pca.explained_variance_            # length = n_components\n",
    "    var_ratios  = pca.explained_variance_ratio_\n",
    "    comps       = pca.components_                    # shape (n_components, n_features)\n",
    "\n",
    "    # 2. Compute loadings for retained PCs\n",
    "    #    loading_ij = component_ij * sqrt(eigenval_i)\n",
    "    loadings = comps * np.sqrt(eigenvals[:, None])   # broadcasting\n",
    "    loadings_df = pd.DataFrame(\n",
    "        loadings.round(4).T,\n",
    "        index=cols,\n",
    "        columns=[f\"{index_name}_PC{i+1}\" for i in range(n_components)]\n",
    "    )\n",
    "\n",
    "    # 3. Build output DataFrame\n",
    "    df_out = df.drop(columns=cols).copy()\n",
    "    weights = var_ratios / var_ratios.sum()          # normalise if <1\n",
    "    composite = (scores * weights).sum(axis=1)\n",
    "    df_out[index_name] = composite\n",
    "\n",
    "    return df_out, loadings_df, var_ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7959bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = nepal_df[target_cols].corr(method=\"pearson\")   # default is Pearson\n",
    "corr.style.background_gradient(vmin=-1, vmax=1, cmap=\"coolwarm\")  # red-blue scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ac603",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_df, loadings, var = reduce_to_index(nepal_df, target_cols, index_name='Q0__AGR_PROD__continuous', n_components=2)\n",
    "print(loadings)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43072dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = senegal_df[target_cols].corr(method=\"pearson\")   # default is Pearson\n",
    "corr.style.background_gradient(vmin=-1, vmax=1, cmap=\"coolwarm\")  # red-blue scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4aae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "senegal_df, loadings, var = reduce_to_index(senegal_df, target_cols, index_name='Q0__AGR_PROD__continuous', n_components=2)\n",
    "print(loadings)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56808f99",
   "metadata": {},
   "source": [
    "# Create Resilience Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300faf73",
   "metadata": {},
   "source": [
    "Get access to basic services features for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_abs_features(\n",
    "    df: pd.DataFrame,\n",
    "    lat_col: str = \"lat\",\n",
    "    lon_col: str = \"lon\",\n",
    "    water_thresh: float = 100,   # meters\n",
    "    toilet_thresh: float = 50,    # meters\n",
    "    elec_thresh: float = 100      # meters\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich any DataFrame with (lat,lon) cols by adding:\n",
    "      • abs_water_access  (0/1)\n",
    "      • abs_toilet        (0/1)\n",
    "      • abs_electricity   (0/1)\n",
    "    plus these distance columns (in meters):\n",
    "      dist_water_source, dist_toilet, dist_power_line,\n",
    "      dist_primary_school, dist_health_facility,\n",
    "      dist_livestock_market, dist_agri_market,\n",
    "      dist_public_transport\n",
    "    \"\"\"\n",
    "    # 1. Rename & build GeoDataFrame in EPSG:4326 → project to EPSG:3857 for meters\n",
    "    df2 = df.rename(columns={lat_col: \"lat\", lon_col: \"lon\"}).copy()\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df2,\n",
    "        geometry=gpd.points_from_xy(df2.lon, df2.lat),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(epsg=3857)\n",
    "\n",
    "    def add_osm_distance(gdf, tags: dict, col: str):\n",
    "        # compute lat/lon bbox from projected gdf\n",
    "        north, south, east, west = gdf.to_crs(epsg=4326).total_bounds[[3,1,2,0]]\n",
    "        pad = 0.02  # degrees\n",
    "        north += pad; south -= pad; east += pad; west -= pad\n",
    "        bbox = (west, south, east, north)  # (left, bottom, right, top)\n",
    "\n",
    "        # fetch POIs within bbox; signature is features_from_bbox(bbox, tags) :contentReference[oaicite:1]{index=1}\n",
    "        pois = ox.features_from_bbox(bbox, tags)\n",
    "\n",
    "        if pois.empty:\n",
    "            gdf[col] = np.nan\n",
    "        else:\n",
    "            # merge all geometries to speed up distance queries\n",
    "            geom_union = pois.to_crs(epsg=3857).geometry.unary_union\n",
    "            gdf[col] = gdf.geometry.apply(lambda p: p.distance(geom_union))\n",
    "        return gdf\n",
    "\n",
    "    # 2. Compute each service distance\n",
    "    gdf = add_osm_distance(gdf, {\"amenity\":\"drinking_water\"},     \"dist_water_source\")\n",
    "    gdf = add_osm_distance(gdf, {\"amenity\":\"toilets\"},            \"dist_toilet\")\n",
    "    gdf = add_osm_distance(gdf, {\"power\":\"line\"},                 \"dist_power_line\")\n",
    "    gdf = add_osm_distance(gdf, {\"amenity\":\"school\"},             \"dist_primary_school\")\n",
    "    gdf = add_osm_distance(gdf, {\"amenity\":[\"hospital\",\"clinic\"]},\"dist_health_facility\")\n",
    "    gdf = add_osm_distance(gdf, {\"amenity\":\"marketplace\"},        \"dist_livestock_market\")\n",
    "    gdf = add_osm_distance(gdf, {\"shop\":\"greengrocer\"},           \"dist_agri_market\")\n",
    "    # public transport = nearest of bus_station or bus_stop\n",
    "    gdf = add_osm_distance(gdf, {\"amenity\":\"bus_station\"},        \"dist_bus_station\")\n",
    "    gdf = add_osm_distance(gdf, {\"highway\":\"bus_stop\"},           \"dist_bus_stop\")\n",
    "    gdf[\"dist_public_transport\"] = gdf[[\"dist_bus_station\",\"dist_bus_stop\"]].min(axis=1)\n",
    "\n",
    "    # 4. Return original + ABS columns\n",
    "    cols = [\n",
    "      \"dist_water_source\",\"dist_toilet\",\"dist_power_line\",\n",
    "      \"dist_primary_school\",\"dist_health_facility\",\n",
    "      \"dist_livestock_market\",\"dist_agri_market\",\"dist_public_transport\"\n",
    "    ]\n",
    "    df2 = df.rename(columns={\"lat\": lat_col, \"lon\": lon_col}).copy()\n",
    "    return pd.concat([df2.reset_index(drop=True), gdf[cols].reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c40458",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_df_abs = add_abs_features(\n",
    "    nepal_df.copy(),\n",
    "    lat_col=\"Q1__Latitude__continuous\",\n",
    "    lon_col=\"Q1__Longitude__continuous\"\n",
    ")\n",
    "\n",
    "senegal_df_abs = add_abs_features(\n",
    "    senegal_df.copy(),\n",
    "    lat_col=\"Q1__Latitude__continuous\",\n",
    "    lon_col=\"Q1__Longitude__continuous\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf545d",
   "metadata": {},
   "source": [
    "Using access to basic services features and other features create a composite sustainable livelihood score for farmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def _ordinal_to_numeric(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace every *ordered* categorical column by its `.cat.codes`\n",
    "    (-1 becomes NaN so it is picked up by the later fillna).\n",
    "    \"\"\"\n",
    "    for col in frame.select_dtypes(include=\"category\").columns:\n",
    "        if not frame[col].cat.ordered:\n",
    "            raise TypeError(\n",
    "                f\"Column {col} is categorical but not ordered; \"\n",
    "                \"decide whether to one-hot encode, drop, or reorder it.\"\n",
    "            )\n",
    "        # codes: first category→0, second→1, …; missing→-1\n",
    "        frame[col] = (\n",
    "            frame[col]\n",
    "            .cat.codes                     # integers, -1 for NaN\n",
    "            .replace(-1, pd.NA)            # keep missing values explicit\n",
    "            .astype(\"Float64\")             # nullable float\n",
    "        )\n",
    "    return frame\n",
    "\n",
    "def compute_sustainable_livelihood_score(\n",
    "    df: pd.DataFrame,\n",
    "    country: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute a 0–1 'sustainable_livelihood_score' as a formative composite\n",
    "    across five livelihood domains, using PCA(1+2) per domain where needed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    country = country.lower()\n",
    "    \n",
    "    # ── 1. Domain → feature lists ──────────────────────────────────────────\n",
    "    if country == \"nepal\":\n",
    "        domain_features = {\n",
    "            \"natural\": [\n",
    "                \"Q50__How_much_land_that_is_yours_do_you_cultivate_bigha__continuous\",\n",
    "                \"Q51__How_much_land_that_is_rented_or_leased_do_you_cultivate_bigha_log__continuous\",\n",
    "                \"Q52__On_how_much_land_do_you_grow_vegetables_bigha_log__continuous\",\n",
    "                \"Q62__How_much_VEGETABLES_do_you_harvest_per_year_from_this_plot_kilograms_log__continuous\",\n",
    "                \"Q0__TOTAL_AREA_sqrt__continuous\"\n",
    "            ],\n",
    "            \"physical\": [\n",
    "                \"Q55__For_Vegetables_do_you_use_improved_and_or_variety_seeds_yes__binary__1\",\n",
    "                \"Q57__If_you_self_prepare_seedlings_how_Nursery__binary__1\",\n",
    "                \"Q60__do_you_use_pesticides_or_herbicides_on_this_plot_Yes__binary__1\",\n",
    "                \"Q78__How_many_years_have_you_been_using_drip_irrigation_if_no_zero__binary__1\",\n",
    "                \"dist_water_source\",\"dist_toilet\",\"dist_power_line\",\n",
    "                \"dist_primary_school\",\"dist_health_facility\",\n",
    "                \"dist_livestock_market\",\"dist_agri_market\",\"dist_public_transport\"\n",
    "            ],\n",
    "            \"financial\": [\n",
    "                \"Q108__What_is_your_households_yearly_income_from_agriculture_NPR_log__continuous\",\n",
    "                \"Q109__What_is_your_households_yearly_income_overall_including_agriculture_NPR_log__continuous\",\n",
    "                \"Q110__Do_you_currently_have_a_loan_if_no_zero_if_yes_amount_of_money_loaned_NPR__ordinal\",\n",
    "                \"Q70__in_the_past_12_months_did_you_receive_any_info_from_anyone_on_agriculture__binary__1\"\n",
    "            ],\n",
    "            \"human\": [\n",
    "                \"Q101__how_many_people_live_in_this_household_log__continuous\",\n",
    "                \"Q5__AgeYears__continuous\",\n",
    "                \"Q106__Education_level_of_this_person_that_is_interviewed_years_of_formal_education__continuous\",\n",
    "                \"Q107__Education_level_of_your_wife_husband_How_many_years_of_formal_education_log__continuous\"\n",
    "            ],\n",
    "            \"social\": [\n",
    "                \"Q0__hope_total__continuous\",\n",
    "                \"Q0__self_control_score__continuous\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    elif country == \"senegal\":\n",
    "        domain_features = {\n",
    "            \"natural\": [\n",
    "                \"Q60__Land_owned_cultivated_ha__continuous\",\n",
    "                \"Q61__Land_rented_cultivated_ha_log__continuous\",\n",
    "                \"Q62__Land_grow_vegetables_ha__continuous\",\n",
    "                \"Q71__VEG_harvest_per_year_kg_log__continuous\"\n",
    "            ],\n",
    "            \"physical\": [\n",
    "                \"Q69__Use_pesticide_or_herbicide__binary__1\",\n",
    "                \"Q84__Years_using_drip_irrigation_log__continuous\",\n",
    "                \"Q76__Received_agri_info_last_12m__binary__1\",\n",
    "                \"dist_water_source\",\"dist_toilet\",\"dist_power_line\",\n",
    "                \"dist_primary_school\",\"dist_health_facility\",\n",
    "                \"dist_livestock_market\",\"dist_agri_market\",\"dist_public_transport\"\n",
    "            ],\n",
    "            \"financial\": [\n",
    "                \"Q90__Yearly_income_agriculture_XOF_log__continuous\",\n",
    "                \"Q91__Yearly_income_overall_XOF_log__continuous\",\n",
    "                \"Q92__Current_loan_amount_XOF__ordinal\"\n",
    "            ],\n",
    "            \"human\": [\n",
    "                \"Q87__Household_size_sqrt__continuous\",\n",
    "                \"Q5__Age__continuous\",\n",
    "                \"Q0__Education_years_log__continuous\"\n",
    "            ],\n",
    "            \"social\": [\n",
    "                \"Q0__Hope_total__continuous\",\n",
    "                \"Q93__Trust_most_people__ordinal\"\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"country must be 'nepal' or 'senegal'\")\n",
    "\n",
    "    # ── 2. Summarise each domain with PCA(1) or (1+2) ──────────────────────\n",
    "    domain_evr    = {}\n",
    "    domain_scores = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for domain, feats in domain_features.items():\n",
    "        # a) subset, convert ordinals to numeric, fill, standardise\n",
    "        X  = _ordinal_to_numeric(df[feats].copy())\n",
    "        X  = X.fillna(X.mean())                 # now safe: all columns numeric\n",
    "        Xs = StandardScaler().fit_transform(X)\n",
    "\n",
    "        # b) fit two PCs and keep 1 or 1+2 as before\n",
    "        pca                 = PCA(n_components=2, random_state=0).fit(Xs)\n",
    "        evr1, evr2          = pca.explained_variance_ratio_\n",
    "        comp1, comp2        = pca.transform(Xs).T\n",
    "        score               = (evr1 * comp1 + evr2 * comp2) / (evr1 + evr2) \\\n",
    "                              if evr1 < 0.50 else comp1\n",
    "        domain_evr[domain]  = evr1 + evr2 if evr1 < 0.50 else evr1\n",
    "        domain_scores[f\"{domain}_score\"] = score\n",
    "\n",
    "    # ── 3. Composite: variance‐weighted sum of domain scores ───────────────\n",
    "    total_evr = sum(domain_evr.values())\n",
    "    weights   = {d: evr / total_evr for d, evr in domain_evr.items()}\n",
    "\n",
    "    df[\"Q0__sustainable_livelihood_score__continuous\"] = \\\n",
    "        sum(domain_scores[f\"{d}_score\"] * w for d, w in weights.items())\n",
    "\n",
    "    return df[\"Q0__sustainable_livelihood_score__continuous\"], weights\n",
    "\n",
    "\n",
    "# ── Usage ───────────────────────────────────────────────────────────────\n",
    "nepal_df[\"Q0__sustainable_livelihood_score__continuous\"], nepal_weights   = compute_sustainable_livelihood_score(nepal_df_abs,   country=\"nepal\")\n",
    "senegal_df[\"Q0__sustainable_livelihood_score__continuous\"], senegal_weights = compute_sustainable_livelihood_score(senegal_df_abs, country=\"senegal\")\n",
    "\n",
    "print(nepal_df[[\"Q0__sustainable_livelihood_score__continuous\"]].head())\n",
    "print(f\"{nepal_weights}\")\n",
    "print(senegal_df[[\"Q0__sustainable_livelihood_score__continuous\"]].head())\n",
    "print(f\"{senegal_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_df = drop_non_relevant_columns(nepal_df)\n",
    "senegal_df = drop_non_relevant_columns(senegal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdb925",
   "metadata": {},
   "source": [
    "# Category Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b57f4f",
   "metadata": {},
   "source": [
    "Get all categorical and binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a660ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_nepal = []\n",
    "cat_cols_senegal = []\n",
    "for col in nepal_df.columns:\n",
    "    meta_type = parse_feature_metadata(col)['type']\n",
    "    if meta_type not in ['continuous', 'discrete', 'time']:\n",
    "        cat_cols_nepal.append(col)\n",
    "\n",
    "for col in senegal_df.columns:\n",
    "    meta_type = parse_feature_metadata(col)['type']\n",
    "    if meta_type not in ['continuous', 'discrete', 'time']:\n",
    "        cat_cols_senegal.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bb8cf",
   "metadata": {},
   "source": [
    "Check that none have an overly large amount of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = nepal_df[cat_cols_nepal].nunique()\n",
    "# 3. see which columns are OK (≤20) or not (>20)\n",
    "ok      = unique_counts[ unique_counts <= 20 ]\n",
    "too_many = unique_counts[ unique_counts  > 20 ]\n",
    "\n",
    "print(\"Columns with ≤20 categories:\\n\", ok.sort_values(ascending=False))\n",
    "print(\"\\nColumns with >20 categories:\\n\", too_many.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd44730",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = senegal_df[cat_cols_senegal].nunique()\n",
    "# 3. see which columns are OK (≤20) or not (>20)\n",
    "ok      = unique_counts[ unique_counts <= 20 ]\n",
    "too_many = unique_counts[ unique_counts  > 20 ]\n",
    "\n",
    "print(\"Columns with ≤20 categories:\\n\", ok.sort_values(ascending=False))\n",
    "print(\"\\nColumns with >20 categories:\\n\", too_many.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376278c",
   "metadata": {},
   "source": [
    "Only telephone, village and machinery columns have large amounts of categories which makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9cbe88",
   "metadata": {},
   "source": [
    "# Constant Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619532b",
   "metadata": {},
   "source": [
    "We remove all remaining columns that have only a single value as this adds no new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_df = nepal_df.loc[:, nepal_df.nunique(dropna=False) > 1]\n",
    "senegal_df = senegal_df.loc[:, senegal_df.nunique(dropna=False) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd8cc1-be9a-4c37-9c9a-f155db36bbe6",
   "metadata": {},
   "source": [
    "# Second Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f267f65",
   "metadata": {},
   "source": [
    "Change phone number column to just binary yes no if he has a phone number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43998a-d55a-4492-a0a5-16bbbf475d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nepal_df['Q4__PhoneNumberAndIsZeroIfNone__nominal'] = (nepal_df['Q4__PhoneNumberAndIsZeroIfNone__nominal'] == 0).astype(int)\n",
    "senegal_df['Q4__Phone_Number__nominal'] = (senegal_df['Q4__Phone_Number__nominal'] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66175a1-0a27-4daa-9b14-a223f27bed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepal_df.rename(columns={'Q4__PhoneNumberAndIsZeroIfNone__nominal' : 'Q4__HasPhoneNumber__binary__1'}, inplace=True)\n",
    "senegal_df.rename(columns={'Q4__Phone_Number__nominal' : 'Q4__HasPhoneNumber__binary__1'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca692a9",
   "metadata": {},
   "source": [
    "Scale all numeric columns and one hot encode all categorical columns, and deal with missing values in categorical columns by adding a 'missing' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba8311-2a30-4a81-bacc-7a6cb451805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import re\n",
    "\n",
    "# def scale_numeric_columns(df, scale_ordinal=True):\n",
    "#     df = df.copy()\n",
    "#     # Get non-categorical numeric columns\n",
    "#     cols_to_scale = [col for col in df.columns if parse_feature_metadata(col)[\"type\"] in ['continuous', 'discrete', 'ordinal' if scale_ordinal else None]]\n",
    "\n",
    "#     for col in cols_to_scale:\n",
    "#         median = df[col].median()\n",
    "#         # this returns a new Series (up-cast to float64) and assigns it back\n",
    "#         df[col] = df[col].fillna(median)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def one_hot_encode_columns(df):\n",
    "#     df = df.copy()\n",
    "#     cols_to_dummies = []\n",
    "#     cols_to_dummies_prefixes = []\n",
    "#     for col in df.columns:\n",
    "#         meta = parse_feature_metadata(col)\n",
    "#         if meta[\"type\"] in ['ordinal', 'nominal']:\n",
    "#             # add it to columns to one hot encode along with its question number\n",
    "#             cols_to_dummies_prefixes.append(f\"{meta['qid']}__{meta['name']}\")\n",
    "#             cols_to_dummies.append((col, meta[\"type\"]))\n",
    "    \n",
    "#     col_names_to_dummies, _ = zip(*cols_to_dummies) \n",
    "#     col_names_to_dummies = list(col_names_to_dummies)\n",
    "#     # applying prefix to dummies according to our feature name format\n",
    "#     dummies = pd.get_dummies(df[col_names_to_dummies], prefix_sep='_', prefix=cols_to_dummies_prefixes, columns=col_names_to_dummies, dtype=int)\n",
    "    \n",
    "#     def reformat(col):\n",
    "#         qid, name = col.split(\"__\")\n",
    "#         return f\"{qid}__{re.sub(r'[^A-Za-z0-9_]', '', name)}\"\n",
    "\n",
    "#     dummies = dummies.rename(columns=reformat)\n",
    "\n",
    "#     # applying postfix to dummies according to our feature name format\n",
    "#     rename_map = {}\n",
    "#     for orig, type in cols_to_dummies:\n",
    "#         # find all dummy cols that start with \"<orig's qid>-<orig's name>\"\n",
    "#         group = [c for c in dummies.columns if c.startswith(f\"{parse_feature_metadata(orig)['qid']}__{parse_feature_metadata(orig)['name']}\")]\n",
    "#         # enumerate them 1,2,3…\n",
    "#         for i, col_name in enumerate(group, start=1):\n",
    "#             rename_map[col_name] = f\"{col_name}__binary_{type}__{i}\"\n",
    "            \n",
    "#     dummies = dummies.rename(columns=rename_map)\n",
    "#     df_other = df.drop(columns=col_names_to_dummies)\n",
    "#     final_df = pd.concat([df_other, dummies], axis=1)\n",
    "    \n",
    "#     return final_df\n",
    "\n",
    "def get_nominals(df):\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        if parse_feature_metadata(col)['type'] == 'nominal':\n",
    "            cols.append(col)\n",
    "    return cols\n",
    "\n",
    "def convert_ordinal_to_numeric(df):\n",
    "    for col in df.columns:\n",
    "        if parse_feature_metadata(col)['type'] == 'ordinal' and df[col].map(type).unique()[0] == str:\n",
    "            if df[col].str.match(r'.*\\d$').all():\n",
    "                df[col] = df[col].apply(lambda x: int(x[-1]))\n",
    "            elif df[col].cat.ordered:\n",
    "                df[col] = df[col].cat.codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a43faf",
   "metadata": {},
   "source": [
    "Supervisor asked us to not use specific psychological indicators in our final method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f974c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NEPAL_HOPE_QUESTIONS_RANGE = list(range(17,29))\n",
    "NEPAL_FEELINGS_QUESTIONS_RANGE = list(range(30, 50))\n",
    "NEPAL_SELF_CONTROL_QUESTIONS_RANGE = list(range(80, 100))\n",
    "NEPAL_PSYCH_QUESTION_RANGE = NEPAL_HOPE_QUESTIONS_RANGE \\\n",
    "                            + NEPAL_FEELINGS_QUESTIONS_RANGE \\\n",
    "                            + NEPAL_SELF_CONTROL_QUESTIONS_RANGE\n",
    "\n",
    "SENEGAL_HOPE_QUESTIONS_RANGE = list(range(11,23))\n",
    "SENEGAL_CHARACTER_STRENGTH_QUESTIONS_RANGE = list(range(24, 60))\n",
    "SENEGAL_PSYCH_QUESTION_RANGE = SENEGAL_HOPE_QUESTIONS_RANGE \\\n",
    "                                + SENEGAL_CHARACTER_STRENGTH_QUESTIONS_RANGE\n",
    "# Q51 only used for prod/sust indices, converting to ordinal and dropping log transformed one since produces MAD zero when calculating bicor\n",
    "\n",
    "NEPAL_ADDITIONAL = [\"Q0__positive_total__continuous\", \"Q0__negative_total__continuous\", \"Q1__Latitude__continuous\", \"Q1__Longitude__continuous\", \"Q1__Accuracy__continuous\",\n",
    "                    \"Q78__How_many_years_have_you_been_using_drip_irrigation_if_no_zero__binary__1\",\n",
    "                    \"Q51__How_much_land_that_is_rented_or_leased_do_you_cultivate_bigha_log__continuous\"]\n",
    "SENEGAL_ADDITIONAL = []\n",
    "# NEPAL_ADDITIONAL = [\"Q1__Latitude__continuous\", \"Q1__Longitude__continuous\", \"Q1__Accuracy__continuous\", \"Q50__How_much_land_that_is_yours_do_you_cultivate_bigha__continuous\",\n",
    "#                     \"Q51__How_much_land_that_is_rented_or_leased_do_you_cultivate_bigha__continuous\", \"Q62__How_much_VEGETABLES_do_you_harvest_per_year_from_this_plot_kilograms__continuous\",\n",
    "#                     \"Q108__What_is_your_households_yearly_income_from_agriculture_NPR__continuous\", \"Q52__On_how_much_land_do_you_grow_vegetables_bigha__continuous\",\n",
    "#                     \"Q0__positive_total__continuous\", \"Q0__negative_total__continuous\"]\n",
    "# SENEGAL_ADDITIONAL = [\"Q1__Latitude__continuous\", \"Q1__Longitude__continuous\", \"Q1__Accuracy__continuous\", \"Q60__Land_owned_cultivated_ha__continuous\", \"Q61__Land_rented_cultivated_ha__continuous\",\n",
    "#                       \"Q71__VEG_harvest_per_year_kg__continuous\", \"Q90__Yearly_income_agriculture_XOF__continuous\", \"Q69__Use_pesticide_or_herbicide__binary__1\", \"Q0__Distance_Thies_KM__continuous\",\n",
    "#                       \"Q0__Distance_Dakar_KM__continuous\", \"Q62__Land_grow_vegetables_ha__continuous\"]\n",
    "\n",
    "def drop_ordinal_psych_columns(df, country):\n",
    "    for col in df.columns:\n",
    "        qid = int(parse_feature_metadata(col)[\"qid\"].replace('Q', ''))\n",
    "        if country == \"NEPAL\" and (qid in NEPAL_PSYCH_QUESTION_RANGE or col in NEPAL_ADDITIONAL):\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        if country == \"SENEGAL\" and (qid in SENEGAL_PSYCH_QUESTION_RANGE or col in SENEGAL_ADDITIONAL):\n",
    "            df.drop(col, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize continuous features to mitigate extreme outliers\n",
    "for df in (nepal_df, senegal_df):\n",
    "    cont_cols = [c for c in df.columns if c.endswith(\"__continuous\")]\n",
    "    q1 = df[cont_cols].quantile(0.01)\n",
    "    q99 = df[cont_cols].quantile(0.99)\n",
    "    df[cont_cols] = df[cont_cols].clip(lower=q1, upper=q99, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_ordinal_to_numeric(nepal_df)\n",
    "\n",
    "drop_ordinal_psych_columns(nepal_df, \"NEPAL\")\n",
    "nepal_df.to_excel(\"nepal_dataframe_FA.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d1faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_ordinal_to_numeric(senegal_df)\n",
    "\n",
    "drop_ordinal_psych_columns(senegal_df, \"SENEGAL\")\n",
    "senegal_df.to_excel(\"senegal_dataframe_FA.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smallholder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
